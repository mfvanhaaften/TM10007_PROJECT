{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "outputs": [],
   "source": [
    "# TM10007 Assignment: Prediction of Tumor Grade in Brain Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 4: Kiefer Comassi (4402359), Myrthe van Haaften (4547470), Frédérique Koopman (4470885), Stephanie Stoutjesdijk (4557808)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the following sections:\n",
    "\n",
    "1. Installing and importing functions and packages\n",
    "\n",
    "2. Loading and splitting data\n",
    "3. Preprocessing before crossvalidation\n",
    "\n",
    "   3.1 Overview of NaN's in the dataset\n",
    "\n",
    "   3.2 Feature removal based on the number of NaN's\n",
    "\n",
    "   3.3 Patient removal based on the number of NaN's\n",
    "   \n",
    "   3.4 Evaluation of data distribution and outliers\n",
    "\n",
    "4. Function definitions\n",
    "\n",
    "    4.1 Imputation\n",
    "    \n",
    "    4.2 Scaling\n",
    "\n",
    "    4.3 Feature selection/dimensionality reduction\n",
    "\n",
    "    4.4 Hyperparameter optimization feature selection method\n",
    "\n",
    "    4.5 Randomized grid searches\n",
    "\n",
    "    4.6 Performance metrics\n",
    "\n",
    "    4.7 Learning curves\n",
    "\n",
    "5. Evaluation of feature selection methods\n",
    "\n",
    "6. Outer and inner crossvalidation\n",
    "\n",
    "7. Performance of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing and importing functions and packages"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2:80: E501 line too long (87 > 79 characters)\n2:80: E501 line too long (87 > 79 characters)\n"
    }
   ],
   "source": [
    "# Run this to use from colab environment\n",
    "# !pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git\n",
    "# !pip install missingpy\n",
    "# !pip install flake8 pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "13:77: W291 trailing whitespace\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import shapiro, uniform\n",
    "from sklearn.model_selection import (\n",
    "                                    StratifiedKFold, StratifiedShuffleSplit,\n",
    "                                    RandomizedSearchCV, learning_curve)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import (\n",
    "                                    RFECV, SelectKBest, mutual_info_classif, \n",
    "                                    SelectFromModel)\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from missingpy import KNNImputer\n",
    "\n",
    "from brats.load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pycodestyle_on\n",
    "#%pycodestyle_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and splitting data "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "11:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA = load_data()\n",
    "\n",
    "# Splitting data into feature values and patient labels\n",
    "FEATURES = DATA.drop(columns=['label'])\n",
    "LABELS = DATA['label']\n",
    "\n",
    "GBM = FEATURES.loc[LABELS == 'GBM']\n",
    "LGG = FEATURES.loc[LABELS == 'LGG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing before crossvalidation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Overview of NaN's in the dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "13:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Determining the number of NaN's\n",
    "\n",
    "# Number of NaN's per patient for GBM and LGG patients\n",
    "NO_NAN_ROW_TOTAL = FEATURES.isnull().sum(axis=1)\n",
    "# Number of NaN's per feature for GBM and LGG patients\n",
    "NO_NAN_COL_TOTAL = FEATURES.isnull().sum(axis=0)\n",
    "\n",
    "# Number of NaN's per feature for GBM patients\n",
    "GBM_NO_NAN_COL = GBM.isnull().sum(axis=0)\n",
    "# Number of NaN's per feature for LGG patients\n",
    "LGG_NO_NAN_COL = LGG.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Feature removal based on the number of NaN's. \n",
    "\n",
    "Threshold = the maximum number of NaN's in a column, above which a feature is removed from the dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2:80: E501 line too long (104 > 79 characters)\n21:80: E501 line too long (87 > 79 characters)\n23:80: E501 line too long (94 > 79 characters)\n24:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Define percentage of patients with no data for a certain feature, above which the feature is discarded\n",
    "PERC_MISSING_GBM = 30\n",
    "PERC_MISSING_LGG = 30\n",
    "\n",
    "# Define threshold based on number of NaN's for discarding features\n",
    "THRESHOLD_GBM = floor((PERC_MISSING_GBM/100) * len(GBM.index))\n",
    "THRESHOLD_LGG = floor((PERC_MISSING_LGG/100) * len(LGG.index))\n",
    "\n",
    "# Initialisation\n",
    "FEATURES_REMOVED = []\n",
    "\n",
    "# Append names of features that should be removed\n",
    "for feature in GBM_NO_NAN_COL[GBM_NO_NAN_COL > THRESHOLD_GBM].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "for feature in LGG_NO_NAN_COL[LGG_NO_NAN_COL > THRESHOLD_LGG].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "# Remove features from dataset\n",
    "DATA_FEAT_SEL = FEATURES.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "\n",
    "print(f'{len(DATA_FEAT_SEL.columns)} of {len(FEATURES.columns)} features are left in dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Patient removal based on the number of NaN's. \n",
    "\n",
    "Threshold = the maximum number of NaN's in a row, above which the patient is removed from the dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "4:80: E501 line too long (80 > 79 characters)\n13:80: E501 line too long (81 > 79 characters)\n16:80: E501 line too long (83 > 79 characters)\n24:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Define threshold of number of NaN's above which a patient is removed\n",
    "PERC_MISSING_SAMPLE = 30\n",
    "THRESHOLD_SAMPLE = floor((PERC_MISSING_SAMPLE/100) * len(DATA_FEAT_SEL.columns))\n",
    "\n",
    "# Number of NaN's per patient after feature removal\n",
    "NO_NAN_ROW_FEAT_SEL = DATA_FEAT_SEL.isnull().sum(axis=1)\n",
    "\n",
    "# Initialisation\n",
    "SAMPLES_REMOVED = []\n",
    "LABELS_SEL = LABELS\n",
    "\n",
    "# Looping over the dataset after feature removal to remove patients with a number\n",
    "# of NaN's above the threshold\n",
    "print('The following sample(s) is/are removed from dataset:')\n",
    "for sample in NO_NAN_ROW_FEAT_SEL[NO_NAN_ROW_FEAT_SEL > THRESHOLD_SAMPLE].index[:]:\n",
    "    if sample:\n",
    "        print(sample)\n",
    "        SAMPLES_REMOVED.append(sample)\n",
    "        DATA_FEAT_SEL = DATA_FEAT_SEL.drop(index=sample)\n",
    "        LABELS_SEL = LABELS_SEL.drop(index=sample)\n",
    "\n",
    "print(f'{len(DATA_FEAT_SEL)} of {len(FEATURES)} samples are left in dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Evaluation of data distribution and outliers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "25:80: E501 line too long (85 > 79 characters)\n37:80: E501 line too long (85 > 79 characters)\n38:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Evaluate data distribution using Shapiro test\n",
    "\n",
    "# Impute GBM and LGG patients separately\n",
    "IMPUTER_GBM = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "IMPUTER_LGG = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "\n",
    "X_GBM = DATA_FEAT_SEL[LABELS_SEL == 'GBM']\n",
    "X_IMP_GBM = IMPUTER_GBM.fit_transform(X_GBM)\n",
    "\n",
    "X_LGG = DATA_FEAT_SEL[LABELS_SEL == 'LGG']\n",
    "X_IMP_LGG = IMPUTER_LGG.fit_transform(X_LGG)\n",
    "\n",
    "# Evaluate distributions of features for GBM patients\n",
    "NO_NON_NORMAL_GBM = 0\n",
    "FEAT_NON_NORM_GBM = list()\n",
    "\n",
    "for index, feature in enumerate(X_IMP_GBM.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value < 0.05:\n",
    "        # Number of non normally distributed features\n",
    "        NO_NON_NORMAL_GBM += 1\n",
    "        FEAT_NON_NORM_GBM.append(index)\n",
    "\n",
    "print(f'In the GBM class, {NO_NON_NORMAL_GBM} features are not normally distributed')\n",
    "\n",
    "# Evaluate distributions of features for LGG patients\n",
    "NO_NON_NORMAL_LGG = 0\n",
    "FEAT_NON_NORM_LGG = list()\n",
    "for index, feature in enumerate(X_IMP_LGG.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value < 0.05:\n",
    "        # Number of non normally distributed features\n",
    "        NO_NON_NORMAL_LGG += 1\n",
    "        FEAT_NON_NORM_LGG.append(index)\n",
    "\n",
    "print(f'In the LGG class, {NO_NON_NORMAL_LGG} features are not normally distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "14:80: E501 line too long (97 > 79 characters)\n21:80: E501 line too long (82 > 79 characters)\n23:80: E501 line too long (82 > 79 characters)\n27:80: E501 line too long (84 > 79 characters)\n35:80: E501 line too long (97 > 79 characters)\n44:80: E501 line too long (85 > 79 characters)\n45:80: E501 line too long (131 > 79 characters)\n46:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Evaluate number of outliers per patient group (LGG and GBM),\n",
    "# depending on the Interquartile Range (IQR)\n",
    "\n",
    "# Evaluate per GBM patient each feature\n",
    "NO_OUTLIERS_GBM = 0\n",
    "\n",
    "# Loop over patients\n",
    "for patient in X_IMP_GBM:\n",
    "    feat_out = 0\n",
    "    # Loop over features\n",
    "    for index, feature in enumerate(patient):\n",
    "        # Determine quartiles and interquartile range (IQR)\n",
    "        q25, q75 = np.percentile(X_IMP_GBM[:, index], 25), np.percentile(X_IMP_GBM[:, index], 75)\n",
    "        iqr = q75 - q25\n",
    "        # Define cut-off for outlier evaluation and lower and upper bounds\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        # Determine whether patient has an exceptional feature value\n",
    "        outlier = ((feature > upper) | (feature < lower))\n",
    "        # Count number of features for which patient has exceptional feature value\n",
    "        feat_out += outlier\n",
    "    # Classify patient as outlier when he has exceptional values for > 70 features\n",
    "    if feat_out > 70:\n",
    "        NO_OUTLIERS_GBM += 1\n",
    "\n",
    "print(f'In the GBM class, {NO_OUTLIERS_GBM} patients are considered to be outliers')\n",
    "\n",
    "# Evaluate per LGG patient each feature\n",
    "NO_OUTLIERS_LGG = 0\n",
    "\n",
    "for patient in X_IMP_LGG:\n",
    "    feat_out = 0\n",
    "    for index, feature in enumerate(patient):\n",
    "        q25, q75 = np.percentile(X_IMP_LGG[:, index], 25), np.percentile(X_IMP_LGG[:, index], 75)\n",
    "        iqr = q75 - q25\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        outlier = ((feature > upper) | (feature < lower))\n",
    "        feat_out += outlier\n",
    "    if feat_out > 70:\n",
    "        NO_OUTLIERS_LGG += 1\n",
    "\n",
    "print(f'In the LGG class, {NO_OUTLIERS_LGG} patients are considered to be outliers.')\n",
    "print(f\"For the complete dataset, this means that {(NO_OUTLIERS_LGG + NO_OUTLIERS_GBM)/(len(DATA_FEAT_SEL)) * 100}% is an outlier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function definitions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "4:80: E501 line too long (104 > 79 characters)\n26:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def knn_impute_train_test_set(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Fit k-nearest neighbor imputer on train set and impute train and test set using this fitted imputer.\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "\n",
    "    Output:\n",
    "    X_train_imp: array-like, shape (n_samples, n_features)\n",
    "                Imputed train data.\n",
    "\n",
    "    X_test_imp: array-like, shape (n_samples, n_features)\n",
    "                Imputed test data.\n",
    "    \"\"\"\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imp = imputer.transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    return X_train_imp, X_test_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "4:80: E501 line too long (89 > 79 characters)\n25:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def scale_train_test_set(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Fit MinMax scaler on train set and scale train and test set using this fitted scaler.\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "\n",
    "    Output:\n",
    "    X_train_scal: array-like, shape (n_samples, n_features)\n",
    "                Scaled train data.\n",
    "\n",
    "    X_test_scal: array-like, shape (n_samples, n_features)\n",
    "                Scaled test data.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scal = scaler.transform(X_train)\n",
    "    X_test_scal = scaler.transform(X_test)\n",
    "    return X_train_scal, X_test_scal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Feature selection/dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "26:80: E501 line too long (88 > 79 characters)\n30:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "%%pycodestyle\n",
    "def select_features_univariate(X_train, y_train, X_test, k_value):\n",
    "    \"\"\"\n",
    "    Univariate feature selection using the Mutual Information Criterion.\n",
    "    Fit on training set and transform training and test set.\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "                Target relative to X_train for classification.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "\n",
    "    Output:\n",
    "    X_train_sel: array-like, shape (n_samples, n_features_selected)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features_selected is the number of selected features.\n",
    "    X_test_sel: array-like, shape (n_samples, n_features_selected)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features_selected is the number of selected features.\n",
    "    selection_method.get_support: array-like\n",
    "                Indices of the feature columns dat were selected\n",
    "    \"\"\"\n",
    "    selection_method = SelectKBest(mutual_info_classif, k=k_value).fit(X_train, y_train)\n",
    "    X_train_sel = selection_method.transform(X_train)\n",
    "    X_test_sel = selection_method.transform(X_test)\n",
    "    return X_train_sel, X_test_sel, selection_method.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "4:80: E501 line too long (88 > 79 characters)\n26:80: E501 line too long (84 > 79 characters)\n30:80: E501 line too long (109 > 79 characters)\n31:80: E501 line too long (96 > 79 characters)\n34:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def select_features_l1(X_train, y_train, X_test, threshold_value):\n",
    "    \"\"\"\n",
    "    L1-based feature selection. Fit on training set and transform training and test set.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "                Target relative to X_train for classification.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    threshold_value: string or float\n",
    "                The threshold value to use for feature selection.\n",
    "\n",
    "    Output:\n",
    "    X_train_sel: array-like, shape (n_samples, n_features_selected)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features_selected is the number of selected features.\n",
    "    X_test_sel: array-like, shape (n_samples, n_features_selected)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features_selected is the number of selected features.\n",
    "    \"\"\"\n",
    "    lsvc = LinearSVC(penalty=\"l1\", dual=False, max_iter=10000).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True, threshold=threshold_value)\n",
    "    X_train_sel = model.transform(X_train)\n",
    "    X_test_sel = model.transform(X_test)\n",
    "    print(f' \\n Number of features selected by L1 feature selection: {len(model.get_support(indices=True))}')\n",
    "    print(f' \\n The following column indices of features were selected by L1 feature selection:'\n",
    "          f'\\n {model.get_support(indices=True)}')\n",
    "    return X_train_sel, X_test_sel, model.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "31:80: E501 line too long (85 > 79 characters)\n32:80: E501 line too long (119 > 79 characters)\n34:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "%%pycodestyle\n",
    "def select_features_rfecv(X_train, y_train, scoring):\n",
    "    \"\"\"\n",
    "    Recursive feature elimination using crossvalidation (rfecv).\n",
    "    Fit on training set and return column indices of selected features.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "                Target relative to X_train for classification.\n",
    "    scoring: string\n",
    "                Scoring metric to be used.\n",
    "\n",
    "    Output:\n",
    "    rfecv.get_support: array-like\n",
    "                Column indices of selected features.\n",
    "    \"\"\"\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    optimal_number_features = list()\n",
    "\n",
    "    # classifications\n",
    "    rfecv = RFECV(\n",
    "        estimator=svc, step=1,\n",
    "        cv=StratifiedKFold(4),\n",
    "        scoring=scoring)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    optimal_number_features.append(rfecv.n_features_)\n",
    "    print(\"\\n Optimal number of features according to rfecv: %d\" % rfecv.n_features_)\n",
    "    print(f' \\n The following column indices of features were selected by RFECV: \\n {rfecv.get_support(indices=True)}')\n",
    "    return rfecv.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "29:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def pca_analysis(X_train, X_test, components):\n",
    "    \"\"\"\n",
    "    Conduct PCA-analysis. Fit on train set and transform train and test set.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "            Test vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "    component: int\n",
    "            The number of components to be used.\n",
    "\n",
    "    Output:\n",
    "    X_train_pca: array-like, shape (n_samples, n_components)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_components is the number of components.\n",
    "    X_test_pca: array-like, shape (n_samples, n_components)\n",
    "            Test vector, where n_samples is the number of samples and\n",
    "            n_components is the number of components.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=components)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Hyperparameter optimization feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2:80: E501 line too long (86 > 79 characters)\n6:80: E501 line too long (83 > 79 characters)\n7:80: E501 line too long (81 > 79 characters)\n8:80: E501 line too long (88 > 79 characters)\n9:80: E501 line too long (85 > 79 characters)\n10:80: E501 line too long (99 > 79 characters)\n19:80: E501 line too long (84 > 79 characters)\n22:80: E501 line too long (86 > 79 characters)\n23:80: E501 line too long (86 > 79 characters)\n32:80: E501 line too long (87 > 79 characters)\n35:80: E501 line too long (116 > 79 characters)\n36:80: E501 line too long (116 > 79 characters)\n46:80: E501 line too long (120 > 79 characters)\n47:80: E501 line too long (110 > 79 characters)\n48:80: E501 line too long (97 > 79 characters)\n49:77: E127 continuation line over-indented for visual indent\n49:80: E501 line too long (103 > 79 characters)\n50:80: E501 line too long (119 > 79 characters)\n51:80: E501 line too long (121 > 79 characters)\n52:80: E501 line too long (102 > 79 characters)\n54:80: E501 line too long (104 > 79 characters)\n56:80: E501 line too long (90 > 79 characters)\n63:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "%%pycodestyle\n",
    "def select_hyperparameter_feature_selection(X_train, y_train, hyperparameter_options):\n",
    "    \"\"\"\n",
    "    Determines optimal value for a hyperparameter, e.g. the number of features\n",
    "    to be selected (k), of the univariate feature selection method. The train\n",
    "    set that is provided as input is dividied into train and test sets using K-Fold\n",
    "    crossvalidation. The choice for the best hyperparameter value is based on the\n",
    "    performance of a KNN-classifier trained with the features selected by the univariate\n",
    "    feature selection. The hyperparameter value that results in the highest mean Area\n",
    "    under the Receiver Operator Curve (AUC) score across the 5 folds, is selected as optimal value.\n",
    "\n",
    "    Input:\n",
    "    X-train: X_train : array-like, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "            Target relative to X_train for classification.\n",
    "\n",
    "    hyperparameter_options: list of settings for the hyperparameter to be evaluated.\n",
    "\n",
    "    Output:\n",
    "    param_optimal: float or int, based on the variable types in hyperparameter_options\n",
    "                The value for the hyperparameter that results in the highest mean AUC.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation\n",
    "    clf_inner = KNeighborsClassifier(n_neighbors=15, weights=\"distance\")\n",
    "    skf_inner = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    performance_param_inner = pd.DataFrame()\n",
    "\n",
    "    # Inner crossvalidation\n",
    "    for train_index_inner, validation_index_inner in skf_inner.split(X_train, y_train):\n",
    "\n",
    "        # Train and validation split\n",
    "        X_train_inner, X_val_inner = np.array(X_train)[train_index_inner], np.array(X_train)[validation_index_inner]\n",
    "        y_train_inner, y_val_inner = np.array(y_train)[train_index_inner], np.array(y_train)[validation_index_inner]\n",
    "\n",
    "        # Initialisation\n",
    "        acc_inner = dict()\n",
    "        bal_acc_inner = dict()\n",
    "        roc_auc_score_inner = dict()\n",
    "\n",
    "        # Loop over different values for hyperparameter\n",
    "        for param in hyperparameter_options:\n",
    "            X_train_sel_inner, X_val_sel_inner, selected_indices_inner = \\\n",
    "                                                                select_features_univariate(X_train_inner, y_train_inner,\n",
    "                                                                                           X_val_inner, param)\n",
    "            acc_inner[str(param)], bal_acc_inner[str(param)], roc_auc_score_inner[str(param)] = \\\n",
    "                                                                            get_performance_classifier(\n",
    "                                                                                          clf_inner, X_train_sel_inner,\n",
    "                                                                                          y_train_inner, X_val_sel_inner,\n",
    "                                                                                          y_val_inner)\n",
    "\n",
    "        performance_param_inner = performance_param_inner.append(roc_auc_score_inner, ignore_index=True)\n",
    "\n",
    "    print(f'The performances of different hyperparameter-values across the five folds is:'\n",
    "          f'\\n{performance_param_inner}')\n",
    "\n",
    "    # Compute mean performance and choose optimal value for hyperparameter\n",
    "    mean_performance_param_inner = performance_param_inner.mean()\n",
    "    param_optimal = int(mean_performance_param_inner.idxmax(axis=1))\n",
    "    return param_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Randomized grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "30:80: E501 line too long (101 > 79 characters)\n34:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def rf_randomized_search(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform a Randomized Grid Search on the training set to find\n",
    "    the optimal hyperparameters for the RF classifier.\n",
    "    Hyperparameters that are tuned are: n_estimators, max_features,\n",
    "    min_samples_split, max_depth and min_samples_leaf.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "        Target relative to X_train for classification.\n",
    "\n",
    "    Output:\n",
    "    rf_classifier: estimator\n",
    "        Estimator that was chosen by the search, i.e. estimator\n",
    "        which gave highest score on the left out data.\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {\n",
    "        'n_estimators': [32, 64, 128],\n",
    "        'max_features': ['sqrt', 'log2', 0.10, 0.25, 0.50],\n",
    "        'min_samples_split': [2, 4, 6],\n",
    "        'max_depth': [1, 6, 15, 28],\n",
    "        'min_samples_leaf': [0.05, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    grid = RandomizedSearchCV(RandomForestClassifier(), parameters, refit=True, verbose=0, n_iter=20)\n",
    "    grid.fit(X_train, y_train)\n",
    "    rf_classifier = grid.best_estimator_\n",
    "    return rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "23:80: E501 line too long (99 > 79 characters)\n27:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def knn_randomized_search(X_train, y_train):\n",
    "    \"\"\"Perform a Randomized Grid Search on the training set to find\n",
    "    the optimal hyperparameters for the KNN-classifier.\n",
    "    Hyperparameters that are tuned are: n_neighbors and weights.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "        Target relative to X_train for classification.\n",
    "\n",
    "    Output:\n",
    "    knn_classifier: estimator\n",
    "        Estimator that was chosen by the search, i.e. estimator\n",
    "        which gave highest score on the left out data.\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {'n_neighbors': list(range(15, 20)),\n",
    "                  'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "    grid = RandomizedSearchCV(KNeighborsClassifier(), parameters, refit=True, verbose=0, n_iter=10)\n",
    "    grid.fit(X_train, y_train)\n",
    "    knn_classifier = grid.best_estimator_\n",
    "    return knn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "27:80: E501 line too long (98 > 79 characters)\n31:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def svm_randomized_search(X_train, y_train):\n",
    "    \"\"\"Perform a Randomized Search on the training set to find\n",
    "    the optimal hyperparameters for the SVM classifier.\n",
    "    Hyperparameters that are tuned are: kernel, C, gamma, degree\n",
    "    and coef0.\n",
    "\n",
    "    Input:\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "        Target relative to X_train for classification.\n",
    "\n",
    "    Output:\n",
    "    svm_classifier: estimator\n",
    "        Estimator that was chosen by the search, i.e. estimator\n",
    "        which gave highest score on the left out data.\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = {'kernel': ['linear', 'rbf', 'poly'],\n",
    "                  'C': uniform(loc=0.01, scale=100),\n",
    "                  'gamma': ['scale', 'auto'],\n",
    "                  'degree': [1, 2, 3, 4, 5],\n",
    "                  'coef0': uniform(loc=0.01, scale=19.99)}\n",
    "\n",
    "    grid = RandomizedSearchCV(SVC(probability=True), parameters, refit=True, verbose=0, n_iter=20)\n",
    "    grid.fit(X_train, y_train)\n",
    "    svm_classifier = grid.best_estimator_\n",
    "    return svm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "45:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "def get_performance_classifier(classifier, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train classifier and obtain its performance on an independent test set.\n",
    "\n",
    "    Input:\n",
    "    classifier: classifier object that can be fitted to training data.\n",
    "\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    y_train: array-like, shape (n_samples)\n",
    "             Target relative to X_train for classification.\n",
    "    X_test : array-like, shape (n_samples, n_features)\n",
    "                Test vector, where n_samples is the number of samples and\n",
    "                n_features is the number of features.\n",
    "    y_test: array-like, shape (n_samples)\n",
    "                Target relative to X_test for classification.\n",
    "\n",
    "    Output:\n",
    "    accuracy: float\n",
    "                Accuracy of classifier.\n",
    "\n",
    "    balanced_acc: float\n",
    "                Balanced accuracy of classifier.\n",
    "\n",
    "    roc_auc: float64\n",
    "                Area under the receiver operator curve.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    prediction = classifier.predict(X_test)\n",
    "\n",
    "    # Obtain probabilities of prediction\n",
    "    order_classes = list(classifier.classes_)\n",
    "    positive_class = order_classes.index('LGG')\n",
    "    probability = classifier.predict_proba(X_test)[:, positive_class]\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "    balanced_acc = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, probability)\n",
    "\n",
    "    return accuracy, balanced_acc, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7 Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "93:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Input:\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "    Output:\n",
    "    Plot of learning curve\n",
    "    \"\"\"\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                      train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                      color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                      test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                      color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "              label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "              label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation of feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2:80: E501 line too long (114 > 79 characters)\n14:80: E501 line too long (95 > 79 characters)\n15:80: E501 line too long (89 > 79 characters)\n25:80: E501 line too long (100 > 79 characters)\n26:80: E501 line too long (98 > 79 characters)\n28:80: E501 line too long (92 > 79 characters)\n29:80: E501 line too long (85 > 79 characters)\n37:80: E501 line too long (81 > 79 characters)\n41:80: E501 line too long (225 > 79 characters)\n48:80: E501 line too long (92 > 79 characters)\n49:80: E501 line too long (89 > 79 characters)\n51:80: E501 line too long (80 > 79 characters)\n52:80: E501 line too long (91 > 79 characters)\n58:80: E501 line too long (84 > 79 characters)\n60:80: E501 line too long (83 > 79 characters)\n61:80: E501 line too long (94 > 79 characters)\n63:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# -------------------------- Cross Validation Feature Selection---------------------------------------------------\n",
    "skf_eval_sel = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "DICT_UNI = {'20': [], '35': [], '50': []}\n",
    "DICT_RFECV = {'roc_auc': [], 'accuracy': [], 'balanced_accuracy': []}\n",
    "DICT_L1 = {'1e-05': [], '0.001': [], '0.01': []}\n",
    "DICT_PCA = {'10': [], '20': [], '30': []}\n",
    "FOLD_SEL = 0\n",
    "\n",
    "for train_index, test_index in skf_eval_sel.split(DATA_FEAT_SEL, LABELS_SEL):\n",
    "    FOLD_SEL += 1\n",
    "    print(f'\\n Fold {FOLD_SEL} of evaluation of feature selection methods')\n",
    "    X_train, X_test = np.array(DATA_FEAT_SEL)[train_index], np.array(DATA_FEAT_SEL)[test_index]\n",
    "    y_train, y_test = np.array(LABELS_SEL)[train_index], np.array(LABELS_SEL)[test_index]\n",
    "    X_train_imp, X_test_imp = knn_impute_train_test_set(X_train, X_test)\n",
    "    X_train_scal, X_test_scal = scale_train_test_set(X_train_imp, X_test_imp)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "    # Univariate feature selection\n",
    "    list_uni = [20, 35, 50]\n",
    "    for number in list_uni:\n",
    "        X_train_sel, X_test_sel, selected_indices = select_features_univariate(\n",
    "                                                                              X_train_scal, y_train,\n",
    "                                                                              X_test_scal, number)\n",
    "        acc_uni, bal_acc_uni, roc_uni = get_performance_classifier(\n",
    "                                                                  clf, X_train_sel, y_train,\n",
    "                                                                  X_test_sel, y_test)\n",
    "        DICT_UNI[str(number)].append(roc_uni)\n",
    "        print(f'\\n The following column indices of features were selected by '\n",
    "              f'univariate feature selection: \\n {selected_indices}')\n",
    "\n",
    "    # RFECV feature selection\n",
    "    list_rfecv = ['roc_auc', 'accuracy', 'balanced_accuracy']\n",
    "    for scoring in list_rfecv:\n",
    "        selected_features = select_features_rfecv(X_train_scal, y_train, scoring)\n",
    "        X_train_sel = X_train_scal[:, selected_features]\n",
    "        X_test_sel = X_test_scal[:, selected_features]\n",
    "        acc_rfecv, bal_acc_rfecv, roc_rfecv = get_performance_classifier(\n",
    "                                                                        clf, X_train_sel, y_train,                                                                                                            X_test_sel, y_test)\n",
    "        DICT_RFECV[scoring].append(roc_rfecv)\n",
    "\n",
    "    # L1 feature selection\n",
    "    list_L1 = [0.00001, 0.001, 0.01]\n",
    "    for value in list_L1:\n",
    "        X_train_sel, X_test_sel, selected_indices = select_features_l1(\n",
    "                                                                      X_train_scal, y_train,\n",
    "                                                                      X_test_scal, value)\n",
    "        acc_L1, bal_acc_L1, roc_L1 = get_performance_classifier(\n",
    "                                                               clf, X_train_sel,\n",
    "                                                               y_train, X_test_sel, y_test)\n",
    "        DICT_L1[str(value)].append(roc_L1)\n",
    "\n",
    "    # PCA feature selection\n",
    "    list_pca = [10, 20, 30]\n",
    "    for component in list_pca:\n",
    "        X_train_sel, X_test_sel = pca_analysis(X_train_scal, X_test_scal, component)\n",
    "        acc_pca, bal_acc_pca, roc_pca = get_performance_classifier(\n",
    "                                                                  clf, X_train_sel,\n",
    "                                                                  y_train, X_test_sel, y_test)\n",
    "        DICT_PCA[str(component)].append(roc_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "15:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Evaluate the different feature selection methods by comparing ROC\n",
    "for key, values in DICT_UNI.items():\n",
    "    mean_values = np.mean(values)\n",
    "    print(f'Mean ROC-AUC of univariate, {key} best: {mean_values}')\n",
    "for key, values in DICT_RFECV.items():\n",
    "    mean_values = np.mean(values)\n",
    "    print(f'Mean ROC-AUC of RFECV, {key} scoring: {mean_values}')\n",
    "for key, values in DICT_L1.items():\n",
    "    mean_values = np.mean(values)\n",
    "    print(f'Mean ROC-AUC of L1, threshold of {key}: {mean_values}')\n",
    "for key, values in DICT_PCA.items():\n",
    "    mean_values = np.mean(values)\n",
    "    print(f'Mean ROC-AUC of PCA, {key} components: {mean_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outer and inner crossvalidation "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "18:80: E501 line too long (95 > 79 characters)\n19:80: E501 line too long (89 > 79 characters)\n21:80: E501 line too long (87 > 79 characters)\n25:80: E501 line too long (91 > 79 characters)\n28:80: E501 line too long (92 > 79 characters)\n32:80: E501 line too long (89 > 79 characters)\n37:80: E501 line too long (96 > 79 characters)\n38:80: E501 line too long (105 > 79 characters)\n40:80: E501 line too long (126 > 79 characters)\n47:80: E501 line too long (123 > 79 characters)\n57:80: E501 line too long (122 > 79 characters)\n73:80: E501 line too long (98 > 79 characters)\n74:80: E501 line too long (250 > 79 characters)\n81:80: E501 line too long (81 > 79 characters)\n88:80: E501 line too long (93 > 79 characters)\n105:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "skf_outer = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialization\n",
    "FEATURES_SELECTED_TOTAL = list()\n",
    "FOLD = 0\n",
    "\n",
    "# Create dataframes\n",
    "ACCURACY = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "ACCURACY_BALANCED = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "AUC = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "\n",
    "for train_index, test_index in skf_outer.split(DATA_FEAT_SEL, LABELS_SEL):\n",
    "    FOLD += 1\n",
    "    print(f'\\n \\n Run {FOLD} of outer crossvalidation')\n",
    "\n",
    "    # Split data into train and test set for outer crossvalidation\n",
    "    X_train, X_test = np.array(DATA_FEAT_SEL)[train_index], np.array(DATA_FEAT_SEL)[test_index]\n",
    "    y_train, y_test = np.array(LABELS_SEL)[train_index], np.array(LABELS_SEL)[test_index]\n",
    "\n",
    "    # ---------------------------------- IMPUTATION -----------------------------------\n",
    "\n",
    "    X_train_imp, X_test_imp = knn_impute_train_test_set(X_train, X_test)\n",
    "\n",
    "    # ------------------------------------ SCALING ----------------------------------------\n",
    "    X_train_scal, X_test_scal = scale_train_test_set(X_train_imp, X_test_imp)\n",
    "\n",
    "    # ------------------------------------ FEATURE SELECTION -------------------------------\n",
    "\n",
    "    # Determining optimal number of features k\n",
    "    k_options = [20, 30, 40, 50]\n",
    "    k_optimal = select_hyperparameter_feature_selection(X_train_scal, y_train, k_options)\n",
    "    print(f'The optimal k value for fold {FOLD} is: {k_optimal}')\n",
    "\n",
    "    # Univariate feature selection\n",
    "    X_train_sel, X_test_sel, selected_indices = select_features_univariate(\n",
    "                                                                          X_train_scal, y_train,\n",
    "                                                                          X_test_scal, k_value=k_optimal)\n",
    "\n",
    "    print(f'\\n The following column indices of features were selected by univariate feature selection: \\n {selected_indices}')\n",
    "    # Determine names of features that were selected\n",
    "    features_selected = list()\n",
    "    for index in selected_indices:\n",
    "        features_selected.append(DATA_FEAT_SEL.columns[index])\n",
    "    FEATURES_SELECTED_TOTAL.append(features_selected)\n",
    "\n",
    "    # -------------- INNER CROSSVALIDATION FOR DETERMINATION OF HYPERPARAMETERS OF CLASSIFIERS ----------------------------\n",
    "    svm_classifier = svm_randomized_search(X_train_sel, y_train)\n",
    "\n",
    "    rf_classifier = rf_randomized_search(X_train_sel, y_train)\n",
    "\n",
    "    knn_classifier = knn_randomized_search(X_train_sel, y_train)\n",
    "\n",
    "    clfs = [svm_classifier, rf_classifier, knn_classifier]\n",
    "    clfs_names = ['SVM', 'RF', 'KNN']\n",
    "\n",
    "    # ---------------- TRAIN AND TEST CLASSIFIERS (OUTER CROSSVALIDATION) ------------------------------------------------\n",
    "    # Initialisation\n",
    "    # Accuracy\n",
    "    acc_dict = dict()\n",
    "    accuracy_total = list()\n",
    "\n",
    "    # Balanced accuracy\n",
    "    bal_acc_dict = dict()\n",
    "    bal_acc_total = list()\n",
    "\n",
    "    # AUC\n",
    "    roc_auc_score_dict = dict()\n",
    "    roc_auc_score_total = list()\n",
    "\n",
    "    # Determine performance\n",
    "    for clf, name in zip(clfs, clfs_names):\n",
    "        acc_dict[name], bal_acc_dict[name], roc_auc_score_dict[name] = get_performance_classifier(\n",
    "                                                                                                 clf, X_train_sel, y_train,                                                                                                            X_test_sel, y_test)\n",
    "\n",
    "    accuracy_total.append(acc_dict)\n",
    "    bal_acc_total.append(bal_acc_dict)\n",
    "    roc_auc_score_total.append(roc_auc_score_dict)\n",
    "\n",
    "    ACCURACY = ACCURACY.append(acc_dict, ignore_index=True)\n",
    "    ACCURACY_BALANCED = ACCURACY_BALANCED.append(bal_acc_dict, ignore_index=True)\n",
    "    AUC = AUC.append(roc_auc_score_dict, ignore_index=True)\n",
    "\n",
    "    print(f'Regular accuracy: \\n {acc_dict}')\n",
    "    print(f'Balanced accuracy: \\n {bal_acc_dict}')\n",
    "    print(f'RoOC-AUCscore: \\n {roc_auc_score_dict}')\n",
    "\n",
    "    # -------------------- LEARNING CURVES --------------------------------------------------\n",
    "    # Initialisation\n",
    "    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    num = 0\n",
    "    fig = plt.figure(figsize=(24, 8*len(clfs)))\n",
    "\n",
    "    # Merge train and test set\n",
    "    X_total = np.concatenate((X_train_sel, X_test_sel), axis=0)\n",
    "    y_total = np.concatenate([y_train, y_test])\n",
    "\n",
    "    # Compute learning curves\n",
    "    for clf in clfs:\n",
    "        title = str(type(clf))\n",
    "        ax = fig.add_subplot(7, 3, num+1)\n",
    "        plot_learning_curve(clf, title, X_total, y_total, ax, ylim=(0.3, 1.01),\n",
    "                            cv=cv, train_sizes=np.linspace(0.2, 1, 5))\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "11:1: W391 blank line at end of file\n"
    }
   ],
   "source": [
    "# Determine mean accuracy, balanced accuracy and AUC score per classifier\n",
    "MEAN_ACC = ACCURACY.mean()\n",
    "print(f'The mean accuracy per classifier is: \\n{MEAN_ACC}')\n",
    "MEAN_ACC_BAL = ACCURACY_BALANCED.mean()\n",
    "print(f'The mean balanced accuracy per classifier is: \\n{MEAN_ACC_BAL}')\n",
    "MEAN_AUC = AUC.mean()\n",
    "STD_AUC = AUC.std()\n",
    "print(f'The mean ROC-AUC per classifier is: \\n{MEAN_AUC}')\n",
    "print(f'The standard deviation of ROC-AUC per classifier is: \\n{STD_AUC}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}