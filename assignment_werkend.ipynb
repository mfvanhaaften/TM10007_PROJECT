{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "outputs": [],
   "source": [
    "# TM10007 Assignment: Prediction of Tumor Grade in Brain Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 4: Kiefer Comassi (4402359), Myrthe van Haaften (4547470), Frédérique Koopman (4470885), Stephanie Stoutjesdijk (4557808)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook containt the following sections:\n",
    "\n",
    "1. Installing and importing functions and packages\n",
    "\n",
    "2. Loading and splitting data\n",
    "3. Preprocessing before crossvalidation\n",
    "\n",
    "   3.1 Overview of NaN's in the dataset\n",
    "\n",
    "   3.2 Feature removal based on the number of NaN's\n",
    "\n",
    "   3.3 Patient removal based on the number of NaN's\n",
    "   \n",
    "   3.4 Evaluation of data distribution and outliers\n",
    "\n",
    "4. Function definitions\n",
    "\n",
    "    4.1 Imputation\n",
    "    \n",
    "    4.2 Scaling\n",
    "\n",
    "    4.3 Feature selection/dimensionality reduction\n",
    "\n",
    "    4.4 Hyperparameter optimization feature selection method\n",
    "\n",
    "    4.5 Randomized grid searches\n",
    "\n",
    "    4.6 Performance metrics\n",
    "\n",
    "    4.7 Learning curves\n",
    "\n",
    "5. Evaluation of feature selection methods\n",
    "\n",
    "6. Outer and inner crossvalidation\n",
    "\n",
    "7. Performance of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing and importing functions and packages"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git\n",
    "#!pip install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import shapiro, uniform\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from missingpy import KNNImputer\n",
    "from operator import itemgetter\n",
    "from scipy import mean\n",
    "\n",
    "from brats.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and splitting data "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_data()\n",
    "\n",
    "# Splitting data into feature values and patient labels\n",
    "FEATURES = data.drop(columns=['label'])\n",
    "LABELS = data['label']\n",
    "\n",
    "GBM = FEATURES.loc[LABELS=='GBM']\n",
    "LGG = FEATURES.loc[LABELS=='LGG']\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing before crossvalidation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Overview of the NaN's in the dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of NaN's\n",
    "NO_NAN_ROW_TOTAL = FEATURES.isnull().sum(axis=1)             # Number of NaN's per patient for GBM and LGG patients\n",
    "NO_NAN_COL_TOTAL = FEATURES.isnull().sum(axis=0)             # Number of NaN's per feature for GBM and LGG patients\n",
    "\n",
    "GBM_NO_NAN_COL = GBM.isnull().sum(axis=0)                    # Number of NaN's per feature for GBM patients\n",
    "LGG_NO_NAN_COL = LGG.isnull().sum(axis=0)                    # Number of NaN's per feature for LGG patients\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Feature removal based on the number of NaN's. Threshold = the maximum number of NaN's in a column"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "482/724 features are left in dataset\n"
    }
   ],
   "source": [
    "# Define percentage of patients with no data for a certain feature, above which the feature is discarded\n",
    "PERC_MISSING_GBM = 30\n",
    "PERC_MISSING_LGG = 30\n",
    "\n",
    "# Define threshold of number of NaN's \n",
    "THRESHOLD_GBM = floor((PERC_MISSING_GBM/100) * len(GBM.index))\n",
    "THRESHOLD_LGG = floor((PERC_MISSING_LGG/100) * len(LGG.index))\n",
    "\n",
    "# Initialisation\n",
    "FEATURES_REMOVED = []\n",
    "\n",
    "# Append names of features that should be discarded \n",
    "\n",
    "for feature in GBM_NO_NAN_COL[GBM_NO_NAN_COL > THRESHOLD_GBM].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "for feature in LGG_NO_NAN_COL[LGG_NO_NAN_COL > THRESHOLD_LGG].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "# Remove features from dataset\n",
    "DATA_FEAT_SEL = FEATURES.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "\n",
    "print(f'{len(DATA_FEAT_SEL.columns)}/{len(FEATURES.columns)} features are left in dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Patient removal based on the number of NaN's. Threshold = the maximum number of NaN's in a row."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The following sample(s) is/are removed from dataset:\nTCGA-HT-7686\n166/167 samples are left in dataset\n"
    }
   ],
   "source": [
    "# Define threshold of number of NaN's above which a patient is removed\n",
    "PERC_MISSING_SAMPLE = 30\n",
    "THRESHOLD_SAMPLE = floor((PERC_MISSING_SAMPLE/100) * len(DATA_FEAT_SEL.columns))\n",
    "\n",
    "# Number of NaN's per patient after feature removal\n",
    "NO_NAN_ROW_TRAIN = DATA_FEAT_SEL.isnull().sum(axis=1)      \n",
    "\n",
    "# Initialisation\n",
    "SAMPLES_REMOVED = [] \n",
    "LABELS_SEL = LABELS\n",
    "\n",
    "# Looping over the trainingset to remove patients with a number of NaN's above the threshold\n",
    "print('The following sample(s) is/are removed from dataset:')\n",
    "for sample in NO_NAN_ROW_TRAIN[NO_NAN_ROW_TRAIN > THRESHOLD_SAMPLE].index[:]:\n",
    "    if sample:\n",
    "        print(sample)\n",
    "        SAMPLES_REMOVED.append(sample)      \n",
    "        DATA_FEAT_SEL = DATA_FEAT_SEL.drop(index=sample)\n",
    "        LABELS_SEL = LABELS_SEL.drop(index=sample)\n",
    "\n",
    "print(f'{len(DATA_FEAT_SEL)}/{len(FEATURES)} samples are left in dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Evaluation of data distribution and outliers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data distribution\n",
    "\n",
    "# Impute GBM and LGG patients separately\n",
    "IMPUTER_GBM = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "IMPUTER_LGG = KNNImputer(n_neighbors=5, weights=\"distance\")        \n",
    "\n",
    "X_GBM = DATA_FEAT_SEL[LABELS_SEL=='GBM']        \n",
    "X_IMP_GBM = IMPUTER_GBM.fit_transform(X_GBM)\n",
    "\n",
    "X_LGG = DATA_FEAT_SEL[LABELS_SEL=='LGG']        \n",
    "X_IMP_LGG = IMPUTER_LGG.fit_transform(X_LGG)   \n",
    "\n",
    "# Evaluate distributions of features for GBM patients\n",
    "NO_NON_NORMAL_GBM = 0       \n",
    "FEAT_NON_NORM_GBM = list()\n",
    "\n",
    "for index, feature in enumerate(X_IMP_GBM.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value<0.05:\n",
    "        NO_NON_NORMAL_GBM += 1          # Number of non normally distributed features\n",
    "        FEAT_NON_NORM_GBM.append(index)\n",
    "\n",
    "# Evaluate distributions of features for LGG patients\n",
    "NO_NON_NORMAL_LGG = 0\n",
    "FEAT_NON_NORM_LGG = list()\n",
    "for index, feature in enumerate(X_IMP_LGG.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value<0.05:\n",
    "        NO_NON_NORMAL_LGG += 1          # Number of non normally distributed features\n",
    "        FEAT_NON_NORM_LGG.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate number of outliers per patient group (LGG and GBM), depending on the Interquartile Range (IQR)\n",
    "\n",
    "# Evaluate per GBM patient each feature\n",
    "NO_OUTLIERS_GBM = 0\n",
    "\n",
    "for patient in X_IMP_GBM:                                                                       # Loop over patients\n",
    "    feat_out = 0\n",
    "    for index, feature in enumerate(patient):                                                   # Loop over features\n",
    "        q25, q75 = np.percentile(X_IMP_GBM[:,index], 25), np.percentile(X_IMP_GBM[:,index], 75) # Determine quartiles\n",
    "        iqr = q75 - q25                                                                         # Determine interquartile range\n",
    "        cut_off = iqr * 1.5                                      # Define cut-off for outlier evaluation\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off              # Determine upper and lower bounds of feature \n",
    "        outlier = ((feature>upper) | (feature<lower))            # Determine whether patient has an exceptional feature value\n",
    "        feat_out += outlier                                      # Count number of features for which patient has exceptional feature value\n",
    "    if feat_out > 70:                                               \n",
    "        NO_OUTLIERS_GBM += 1                                     # Classify patient as outlier when he has exceptional values for > 70 features\n",
    "\n",
    "# Evaluate per LGG patient each feature\n",
    "NO_OUTLIERS_LGG = 0\n",
    "\n",
    "for patient in X_IMP_LGG:\n",
    "    feat_out = 0\n",
    "    for index, feature in enumerate(patient):\n",
    "        q25, q75 = np.percentile(X_IMP_LGG[:,index], 25), np.percentile(X_IMP_LGG[:,index], 75)\n",
    "        iqr = q75 - q25\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        outlier = ((feature>upper) | (feature<lower))\n",
    "        feat_out += outlier\n",
    "    if feat_out > 70:\n",
    "        NO_OUTLIERS_LGG += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function definitions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit k-nearest neighbor imputer on train set and impute train and test set using this fitted imputer.\n",
    "Input: \n",
    "X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "X_test : array-like, shape (n_samples, n_features)\n",
    "        Test vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "Output: \n",
    "X_train_imp: array-like, shape (n_samples, n_features)\n",
    "        Imputed train data.\n",
    "\n",
    "X_test_imp: array-like, shape (n_samples, n_features)\n",
    "        Imputed test data.\n",
    "\"\"\"\n",
    "\n",
    "def knn_impute_train_test_set(X_train, X_test):\n",
    "    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imp = imputer.transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    return X_train_imp, X_test_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit MinMax scaler on train set and scale train and test set using this fitted scaler.\n",
    "Input: \n",
    "X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "X_test : array-like, shape (n_samples, n_features)\n",
    "        Test vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "Output: \n",
    "X_train_scal: array-like, shape (n_samples, n_features)\n",
    "        Scaled train data.\n",
    "\n",
    "X_test_scal: array-like, shape (n_samples, n_features)\n",
    "        Scaled test data.\n",
    "\"\"\"\n",
    "\n",
    "def scale_train_test_set(X_train, X_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scal = scaler.transform(X_train)\n",
    "    X_test_scal = scaler.transform(X_test)\n",
    "    return X_train_scal, X_test_scal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Feature selection/dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Univariate feature selection using the Mutual Information Criterion.\n",
    "Input: \n",
    "X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "y_train: array-like, shape (n_samples) \n",
    "        Target relative to X_train for classification.\n",
    "X_test : array-like, shape (n_samples, n_features)\n",
    "        Test vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "Output: \n",
    "X_train_sel: array-like, shape (n_samples, n_features_selected)\n",
    "        Training vector, where n_samples is the number of samples and \n",
    "        n_features_selected is the number of selected features.\n",
    "X_test_sel: array-like, shape (n_samples, n_features_selected)\n",
    "        Test vector, where n_samples is the number of samples and \n",
    "        n_features_selected is the number of selected features.\n",
    "selection_method.get_support: array-like\n",
    "        Indices of the feature columns dat were selected\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def select_features_univariate(X_train, y_train, X_test, k_value):\n",
    "    selection_method = SelectKBest(mutual_info_classif, k=k_value).fit(X_train, y_train)\n",
    "    X_train_sel = selection_method.transform(X_train)\n",
    "    X_test_sel = selection_method.transform(X_test)\n",
    "    return X_train_sel, X_test_sel, selection_method.get_support(indices=True)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using L1 \n",
    "\"\"\"\n",
    "L1-based feature selection. \n",
    "\n",
    "Input:\n",
    "X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "y_train: array-like, shape (n_samples) \n",
    "        Target relative to X_train for classification.\n",
    "X_test : array-like, shape (n_samples, n_features)\n",
    "        Test vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "threshold_value: string or float\n",
    "        The threshold value to use for feature selection.\n",
    "Output:\n",
    "X_train_sel: array-like, shape (n_samples, n_features_selected)\n",
    "        Training vector, where n_samples is the number of samples and \n",
    "        n_features_selected is the number of selected features.\n",
    "X_test_sel: array-like, shape (n_samples, n_features_selected)\n",
    "        Test vector, where n_samples is the number of samples and \n",
    "        n_features_selected is the number of selected features.\n",
    "\"\"\"\n",
    "def select_features_L1(X_train, y_train, X_test, threshold_value):\n",
    "    lsvc = LinearSVC(penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "    model = SelectFromModel(lsvc, prefit=True, threshold=threshold_value)\n",
    "    X_train_sel = model.transform(X_train)\n",
    "    X_test_sel = model.transform(X_test)\n",
    "    return X_train_sel, X_test_sel, model.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using recursive feature elimination\n",
    "\n",
    "def select_features_rfecv(X_train, y_train, scoring):\n",
    "\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    optimal_number_features = list()\n",
    "\n",
    "    # classifications\n",
    "    rfecv = RFECV(\n",
    "        estimator=svc, step=1, \n",
    "        cv=StratifiedKFold(4),\n",
    "        scoring=scoring)\n",
    "    rfecv.fit(X_train, y_train)                               \n",
    "\n",
    "    optimal_number_features.append(rfecv.n_features_)\n",
    "    print(\"Optimal number of features according to rfecv: %d\" % rfecv.n_features_)\n",
    "\n",
    "    return rfecv.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis(X_train, X_test, components):\n",
    "    pca = PCA(n_components=components)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Hyperparameter optimization feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determines optimal value for a hyperparameter, e.g. the number of features \n",
    "to be selected (k), of the univariate feature selection method. The train \n",
    "set that is provided as input is dividied into train and test sets using K-Fold \n",
    "crossvalidation. The choice for the best hyperparameter value is based on the \n",
    "performance of a KNN-classifier trained with the features selected by the univariate \n",
    "feature selection. The hyperparameter value that results in the highest mean Area \n",
    "under the Receiver Operator Curve (AUC) score across the 5 folds, is selected as optimal value.\n",
    "\n",
    "Input:\n",
    "X-train: X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "y_train: array-like, shape (n_samples) \n",
    "        Target relative to X_train for classification.\n",
    "\n",
    "hyperparameter_options: list of settings for the hyperparameter to be evaluated.\n",
    "\n",
    "Output:\n",
    "param_optimal: float or int, based on the variable types in hyperparameter_options\n",
    "            The value for the hyperparameter that resulted in the highest mean AUC.\n",
    "\"\"\"\n",
    "\n",
    "def select_hyperparameter_feature_selection(X_train, y_train, hyperparameter_options):\n",
    "\n",
    "    # Initialisation\n",
    "    clf_inner = KNeighborsClassifier(n_neighbors=15, weights = \"distance\")\n",
    "    skf_inner = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "    performance_param_inner = pd.DataFrame()  \n",
    "\n",
    "    # Inner crossvalidation \n",
    "    for train_index_inner, validation_index_inner in skf_inner.split(X_train, y_train):\n",
    "        # Train and validation split\n",
    "        X_train_inner, X_val_inner = np.array(X_train)[train_index_inner], np.array(X_train)[validation_index_inner]\n",
    "        y_train_inner, y_val_inner = np.array(y_train)[train_index_inner], np.array(y_train)[validation_index_inner]\n",
    "    \n",
    "        #Initialisation\n",
    "        acc_inner = dict()\n",
    "        bal_acc_inner = dict()\n",
    "        roc_auc_score_inner = dict()\n",
    "        \n",
    "        # Loop over different values for hyperparameter\n",
    "        for param in hyperparameter_options:\n",
    "            X_train_sel_inner, X_val_sel_inner, selected_indices_inner  = select_features_L1(X_train_inner,                                                                                         y_train_inner, X_val_inner, param)      \n",
    "            acc_inner[str(param)], bal_acc_inner[str(param)], roc_auc_score_inner[str(param)] = get_performance_classifier(clf_inner, X_train_sel_inner, y_train_inner, X_val_sel_inner, y_val_inner)\n",
    "        \n",
    "        performance_param_inner = performance_param_inner.append(roc_auc_score_inner, ignore_index=True)\n",
    "    \n",
    "    print(f'The performances of different hyperparameter-values across the five folds is: \\n {performance_param_inner}')\n",
    "\n",
    "    # Compute mean performance and choose optimal value for hyperparameter\n",
    "    mean_performance_param_inner = performance_param_inner.mean()\n",
    "    param_optimal = float(mean_performance_param_inner.idxmax(axis=1))\n",
    "    \n",
    "\n",
    "    return param_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Randomized grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search for Random Forrest Classifier\n",
    "\n",
    "def rf_randomized_search(X_train, y_train):\n",
    "    '''Perform a Randomized Search on the training set \n",
    "    to find the optimal hyperparameters.\n",
    "    Input: training data and labels\n",
    "    Output: Random Forest Classifier with optimal hyperparameters'''\n",
    "\n",
    "    parameters = {\n",
    "        'n_estimators': [32, 64, 128, 150],\n",
    "        'max_features': ['sqrt', 'log2', 0.10, 0.25, 0.50],\n",
    "        'min_samples_split': [2, 4, 6],\n",
    "        'max_depth': [1,6,15,28], \n",
    "        'min_samples_leaf': [0.05, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    grid = RandomizedSearchCV(RandomForestClassifier(), parameters, refit=True, verbose=0, n_iter = 20)\n",
    "    grid.fit(X_train, y_train)\n",
    "    rf_classifier = grid.best_estimator_\n",
    "    return rf_classifier\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search for K-Nearest Neighbors Classifier\n",
    "\n",
    "def knn_randomized_search(X_train, y_train):\n",
    "    '''Perform a Randomized Search on the training set \n",
    "    to find the optimal hyperparameters.\n",
    "    Input: training data and labels\n",
    "    Output: K-Nearest Neighbors Classifier with optimal hyperparameters'''\n",
    "\n",
    "    parameters = {  'n_neighbors': list(range(15,20)),\n",
    "                    'weights': [\"uniform\", \"distance\"]\n",
    "                    }\n",
    "\n",
    "    grid = RandomizedSearchCV(KNeighborsClassifier(), parameters, refit=True, verbose=0, n_iter = 10)\n",
    "    grid.fit(X_train, y_train)\n",
    "    knn_classifier = grid.best_estimator_\n",
    "    return knn_classifier\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search for Support Vector Machine Classifier\n",
    "\n",
    "def svm_randomized_search(X_train, y_train):\n",
    "    '''Perform a Randomized Search on the training set \n",
    "    to find the optimal hyperparameters.\n",
    "    Input: training data and labels\n",
    "    Output: SVM Classifier with optimal hyperparameters'''\n",
    "\n",
    "    parameters =  {'kernel': ['linear', 'rbf', 'poly'],\n",
    "                    'C': uniform(loc=0.01, scale=5), \n",
    "                    'gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                    'degree': [1, 2, 3, 4, 5],\n",
    "                    'coef0': uniform(loc=0.01, scale=19.99)}\n",
    "\n",
    "    grid = RandomizedSearchCV(SVC(probability=True), parameters, refit=True, verbose=0, n_iter = 20)\n",
    "    grid.fit(X_train, y_train)\n",
    "    svm_classifier = grid.best_estimator_\n",
    "    print(svm_classifier)\n",
    "    return svm_classifier\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train classifier and obtain its performance on an independent test set. \n",
    "\n",
    "Input:\n",
    "classifier: classifier object that can be fitted to training data.\n",
    "\n",
    "X_train : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "y_train: array-like, shape (n_samples) \n",
    "        Target relative to X_train for classification.\n",
    "X_test : array-like, shape (n_samples, n_features)\n",
    "        Test vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "y_test: array-like, shape (n_samples) \n",
    "        Target relative to X_test for classification.\n",
    "\n",
    "Output:\n",
    "accuracy: float64\n",
    "        Accuracy of classifier.\n",
    "\n",
    "balanced_acc: float64\n",
    "        Balanced accuracy of classifier.\n",
    "\n",
    "roc_auc: float64\n",
    "        Area under the receiver operator curve of classifier.\n",
    "\"\"\"\n",
    "\n",
    "def get_performance_classifier(classifier, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    prediction = classifier.predict(X_test)\n",
    "\n",
    "    # Obtain probabilities of prediction\n",
    "    order_classes = list(classifier.classes_)\n",
    "    positive_class = order_classes.index('LGG')\n",
    "    probability = classifier.predict_proba(X_test)[:,positive_class]\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "    balanced_acc = metrics.balanced_accuracy_score(y_test, prediction)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, probability)\n",
    "\n",
    "    return accuracy, balanced_acc, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7 Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores  = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation of feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Optimal number of features according to rfecv: 20\nOptimal number of features according to rfecv: 20\nOptimal number of features according to rfecv: 20\nOptimal number of features according to rfecv: 20\nOptimal number of features according to rfecv: 12\nOptimal number of features according to rfecv: 1\nOptimal number of features according to rfecv: 79\nOptimal number of features according to rfecv: 6\nOptimal number of features according to rfecv: 72\nOptimal number of features according to rfecv: 17\nOptimal number of features according to rfecv: 5\nOptimal number of features according to rfecv: 8\nOptimal number of features according to rfecv: 8\nOptimal number of features according to rfecv: 14\nOptimal number of features according to rfecv: 14\n"
    }
   ],
   "source": [
    "#-------------------------- Cross Validation Feature Selection---------------------------------------------------\n",
    "skf_eval_sel = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "acc_total_uni = list()\n",
    "bal_acc_total_uni = list()\n",
    "roc_total_uni = list()\n",
    "acc_total_rfecv = list()\n",
    "bal_acc_total_rfecv = list()\n",
    "roc_total_rfecv = list()\n",
    "acc_total_L1 = list()\n",
    "bal_acc_total_L1 = list()\n",
    "roc_total_L1 = list()\n",
    "acc_total_pca = list()\n",
    "bal_acc_total_pca = list()\n",
    "roc_total_pca = list()\n",
    "\n",
    "dict_uni = {'20': [], '35': [], '50': []}\n",
    "dict_rfecv = {'roc_auc': [], 'accuracy': [], 'balanced_accuracy': []}\n",
    "dict_L1 = {'1e-05': [], '0.001': [], '0.01': []}\n",
    "dict_pca = {'5': [], '10': [], '20': []}\n",
    "\n",
    "for train_index, test_index in skf_eval_sel.split(DATA_FEAT_SEL, LABELS_SEL):\n",
    "    X_train, X_test = np.array(DATA_FEAT_SEL)[train_index], np.array(DATA_FEAT_SEL)[test_index]\n",
    "    y_train, y_test = np.array(LABELS_SEL)[train_index], np.array(LABELS_SEL)[test_index]\n",
    "    X_train_imp, X_test_imp = knn_impute_train_test_set(X_train, X_test)\n",
    "    X_train_scal, X_test_scal = scale_train_test_set(X_train_imp, X_test_imp)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "    # selected features univariate\n",
    "    list_uni = (20,35,50)\n",
    "    for number in list_uni:\n",
    "        X_train_sel, X_test_sel, selected_indices = select_features_univariate(X_train_scal, y_train, X_test_scal, number)\n",
    "        acc_uni, bal_acc_uni, roc_uni = get_performance_classifier(clf, X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "        #print(f'The column indices of the selected features for univariate feature selection are: {selected_indices}')\n",
    "        acc_total_uni.append(acc_uni)\n",
    "        bal_acc_total_uni.append(bal_acc_uni)\n",
    "        roc_total_uni.append(roc_uni)\n",
    "\n",
    "        dict_uni[str(number)].append(roc_uni)\n",
    "\n",
    "    #selected features rfecv\n",
    "    list_rfecv = ('roc_auc','accuracy','balanced_accuracy')\n",
    "    for scoring in list_rfecv:\n",
    "        selected_features = select_features_rfecv(X_train_scal, y_train, scoring)\n",
    "        X_train_sel = X_train_scal[:,selected_features]\n",
    "        X_test_sel = X_test_scal[:,selected_features]\n",
    "        acc_rfecv, bal_acc_rfecv, roc_rfecv = get_performance_classifier(clf, X_train_sel, y_train,                                                                                              X_test_sel, y_test)\n",
    "        #print(f'The column indices of the selected features for RFECV feature selection are: {selected_features}')\n",
    "\n",
    "        acc_total_rfecv.append(acc_rfecv)\n",
    "        bal_acc_total_rfecv.append(bal_acc_rfecv)\n",
    "        roc_total_rfecv.append(roc_rfecv)\n",
    "\n",
    "        dict_rfecv[scoring].append(roc_rfecv)\n",
    "\n",
    "    list_L1 = [0.00001, 0.001, 0.01]\n",
    "    for value in list_L1:\n",
    "        X_train_sel, X_test_sel, selected_indices = select_features_L1(X_train_scal, y_train, X_test_scal, value)\n",
    "        acc_L1, bal_acc_L1, roc_L1 = get_performance_classifier(clf, X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "        acc_total_L1.append(acc_L1)\n",
    "        bal_acc_total_L1.append(bal_acc_L1)\n",
    "        roc_total_L1.append(roc_L1)    \n",
    "\n",
    "        dict_L1[str(value)].append(roc_L1)    \n",
    "        #print(f'The column indices of the selected features for L1 feature selection are: {selected_indices}')\n",
    "\n",
    "     #selected features pca\n",
    "    list_pca = (5, 10, 20)\n",
    "    for component in list_pca:\n",
    "       X_train_sel, X_test_sel = pca_analysis(X_train_scal, X_test_scal, component)\n",
    "       acc_pca, bal_acc_pca, roc_pca = get_performance_classifier(clf, X_train_sel, y_train, X_test_sel, y_test)\n",
    "       dict_pca[str(component)].append(roc_pca)\n",
    "\n",
    "       acc_total_pca.append(acc_pca)\n",
    "       bal_acc_total_pca.append(bal_acc_pca)\n",
    "       roc_total_pca.append(roc_pca)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean ROC of univariatie, 20 best: 0.9268803418803419\nMean ROC of univariatie, 35 best: 0.9224297924297924\nMean ROC of univariatie, 50 best: 0.9238095238095237\nMean ROC of RFECV, roc_auc scoring: 0.9087301587301588\nMean ROC of RFECV, accuracy scoring: 0.8954456654456655\nMean ROC of RFECV, balanced_accuracy scoring: 0.886959706959707\nMean ROC of L1, threshold of 1e-05: 0.9019841269841271\nMean ROC of L1, threshold of 0.001: 0.918070818070818\nMean ROC of L1, threshold of 0.01: 0.923998778998779\nMean ROC of PCA, 5 components: 0.8522954822954822\nMean ROC of PCA, 10 components: 0.8831013431013431\nMean ROC of PCA, 20 components: 0.9037484737484738\n"
    }
   ],
   "source": [
    "# Evaluate the different feature selection methods by comparing ROC\n",
    "for key, values in dict_uni.items():\n",
    "    mean_values = mean(values)\n",
    "    print(f'Mean ROC of univariatie, {key} best: {mean_values}')\n",
    "for key, values in dict_rfecv.items():\n",
    "    mean_values = mean(values)\n",
    "    print(f'Mean ROC of RFECV, {key} scoring: {mean_values}')\n",
    "for key, values in dict_L1.items():\n",
    "    mean_values = mean(values)\n",
    "    print(f'Mean ROC of L1, threshold of {key}: {mean_values}')\n",
    "for key, values in dict_pca.items():\n",
    "    mean_values = mean(values)\n",
    "    print(f'Mean ROC of PCA, {key} components: {mean_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outer and inner crossvalidation "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n \n Run 1 of outer crossvalidation\nThe performances of different hyperparameter-values across the five folds is: \n      0.0001     0.001       0.1         1     1e-05\n0  0.847059  0.835294  0.835294  0.917647  0.888235\n1  0.965909  0.965909  0.954545  0.880682  0.954545\n2  0.975000  0.962500  0.950000  0.850000  0.968750\n3  0.843750  0.843750  0.800000  0.818750  0.856250\n4  0.946875  0.968750  0.981250  0.962500  0.950000\nThe optimal threshold value for fold 1 is: 1e-05\nThe column indices of the selected features for fold 1 are: \n [  9  12  17  24  28  33  35  43  47  51  58  76  79  93 110 113 119 121\n 122 129 131 160 197 200 239 336 337 401 415 429 430 455 476 477 478 480\n 481]\nSVC(C=2.253983429490501, break_ties=False, cache_size=200, class_weight=None,\n    coef0=8.076246507936602, decision_function_shape='ovr', degree=1, gamma=10,\n    kernel='poly', max_iter=-1, probability=True, random_state=None,\n    shrinking=True, tol=0.001, verbose=False)\nRegular accuracy: \n {'SVM': 0.8823529411764706, 'RF': 0.9117647058823529, 'KNN': 0.8823529411764706}\nBalanced accuracy: \n {'SVM': 0.9047619047619048, 'RF': 0.9285714285714286, 'KNN': 0.8754578754578755}\nRoc auc score: \n {'SVM': 0.9633699633699635, 'RF': 0.9432234432234432, 'KNN': 0.945054945054945}\n\n \n Run 2 of outer crossvalidation\nThe performances of different hyperparameter-values across the five folds is: \n      0.0001     0.001       0.1         1     1e-05\n0  0.832353  0.814706  0.867647  0.885294  0.814706\n1  0.903409  0.875000  0.897727  0.815341  0.926136\n2  0.920455  0.920455  0.926136  0.926136  0.937500\n3  0.887500  0.887500  0.831250  0.896875  0.868750\n4  1.000000  1.000000  1.000000  1.000000  1.000000\nThe optimal threshold value for fold 2 is: 1e-05\nThe column indices of the selected features for fold 2 are: \n [  0   9  12  17  28  33  35  36  51  57  62  83  88  95 110 112 120 121\n 129 131 155 197 200 250 351 354 367 379 417 427 430 477 478]\nSVC(C=2.0653293201201244, break_ties=False, cache_size=200, class_weight=None,\n    coef0=5.226844501469388, decision_function_shape='ovr', degree=4, gamma=0.1,\n    kernel='linear', max_iter=-1, probability=True, random_state=None,\n    shrinking=True, tol=0.001, verbose=False)\nRegular accuracy: \n {'SVM': 0.7878787878787878, 'RF': 0.7878787878787878, 'KNN': 0.7878787878787878}\nBalanced accuracy: \n {'SVM': 0.7440476190476191, 'RF': 0.7083333333333334, 'KNN': 0.7083333333333334}\nRoc auc score: \n {'SVM': 0.8611111111111112, 'RF': 0.9563492063492064, 'KNN': 0.8670634920634921}\n\n \n Run 3 of outer crossvalidation\nThe performances of different hyperparameter-values across the five folds is: \n      0.0001     0.001       0.1         1     1e-05\n0  0.976471  0.976471  0.941176  0.832353  0.976471\n1  0.964706  0.976471  0.976471  0.952941  0.976471\n2  0.982955  0.971591  0.982955  0.943182  0.971591\n3  0.962500  0.943750  0.956250  0.828125  0.937500\n4  0.987500  0.981250  0.975000  0.918750  0.987500\nThe optimal threshold value for fold 3 is: 0.0001\nThe column indices of the selected features for fold 3 are: \n [  5   8   9  12  17  23  24  28  35  37  43  45  47  52  57  58  63  75\n  93  98 111 114 115 119 120 121 122 132 182 196 250 291 377 401 430 445\n 455 477 481]\nSVC(C=1.418025538136561, break_ties=False, cache_size=200, class_weight=None,\n    coef0=13.264364055891944, decision_function_shape='ovr', degree=3, gamma=1,\n    kernel='linear', max_iter=-1, probability=True, random_state=None,\n    shrinking=True, tol=0.001, verbose=False)\nRegular accuracy: \n {'SVM': 0.8787878787878788, 'RF': 0.8787878787878788, 'KNN': 0.8484848484848485}\nBalanced accuracy: \n {'SVM': 0.8596153846153847, 'RF': 0.8596153846153847, 'KNN': 0.8211538461538461}\nRoc auc score: \n {'SVM': 0.8923076923076924, 'RF': 0.8807692307692309, 'KNN': 0.8865384615384615}\n\n \n Run 4 of outer crossvalidation\nThe performances of different hyperparameter-values across the five folds is: \n      0.0001     0.001       0.1         1     1e-05\n0  0.994118  0.994118  1.000000  0.682353  0.994118\n1  0.861765  0.838235  0.852941  0.817647  0.844118\n2  0.911932  0.914773  0.943182  0.693182  0.900568\n3  0.912500  0.931250  0.937500  0.915625  0.956250\n4  0.828125  0.809375  0.825000  0.837500  0.803125\nThe optimal threshold value for fold 4 is: 0.1\nThe column indices of the selected features for fold 4 are: \n [  9  11  12  23  24  28  43  51  57  58  68  88  93  95 103 118 120 121\n 129 174 179 200 206 208 250 264 296 337 351 367 373 377 379 411 419 429\n 430 445 473 477 478 481]\nSVC(C=1.3311494893143443, break_ties=False, cache_size=200, class_weight=None,\n    coef0=5.275444294486587, decision_function_shape='ovr', degree=1, gamma=10,\n    kernel='poly', max_iter=-1, probability=True, random_state=None,\n    shrinking=True, tol=0.001, verbose=False)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1fc15e5271a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msvm_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_randomized_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_randomized_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mknn_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_randomized_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-1b9fd7ff3df8>\u001b[0m in \u001b[0;36mrf_randomized_search\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    141\u001b[0m             X_idx_sorted=None):\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccp_alpha\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_random_state\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m    130\u001b[0m                  \u001b[0;34m(\u001b[0m\u001b[0m_errdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mSHIFT_OVERFLOW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                  \u001b[0;34m(\u001b[0m\u001b[0m_errdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mSHIFT_UNDERFLOW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                  (_errdict[invalid] << SHIFT_INVALID))\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mpyvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "skf_outer = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "# Initialization\n",
    "FEATURES_SELECTED_TOTAL = list()\n",
    "fold = 0\n",
    "\n",
    "# Create dataframes\n",
    "ACCURACY = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "ACCURACY_BALANCED = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "AUC = pd.DataFrame(columns=['SVM', 'RF', 'KNN'])\n",
    "\n",
    "\n",
    "for train_index, test_index in skf_outer.split(DATA_FEAT_SEL, LABELS_SEL):\n",
    "    fold += 1 \n",
    "    print(f'\\n \\n Run {fold} of outer crossvalidation')\n",
    "\n",
    "    # Split data into train and test set for outer crossvalidation\n",
    "    X_train, X_test = np.array(DATA_FEAT_SEL)[train_index], np.array(DATA_FEAT_SEL)[test_index]\n",
    "    y_train, y_test = np.array(LABELS_SEL)[train_index], np.array(LABELS_SEL)[test_index]\n",
    "\n",
    "\n",
    "    # ---------------------------------- IMPUTATION -----------------------------------\n",
    "\n",
    "    X_train_imp, X_test_imp = knn_impute_train_test_set(X_train, X_test)\n",
    " \n",
    "    # ------------------------------------ SCALING ----------------------------------------\n",
    "    X_train_scal, X_test_scal = scale_train_test_set(X_train_imp, X_test_imp)\n",
    "\n",
    "    # ------------------------------------ FEATURE SELECTION -------------------------------\n",
    "\n",
    "    # Determining optimal number of features k \n",
    "    #k_options = [20, 30, 40, 50]\n",
    "    #k_optimal = select_hyperparameter_feature_selection(X_train_scal, y_train, k_options)\n",
    "    #print(f'The optimal k value for fold {fold} is: {k_optimal}')\n",
    "\n",
    "    threshold_options = [0.00001, 0.0001, 0.001, 0.1, 1]\n",
    "    threshold_optimal = select_hyperparameter_feature_selection(X_train_scal, y_train, threshold_options)\n",
    "    print(f'The optimal threshold value for fold {fold} is: {threshold_optimal}')\n",
    "\n",
    "    # Univariate feature selection\n",
    "    X_train_sel, X_test_sel, selected_indices = select_features_L1(X_train_scal, y_train, X_test_scal, threshold_value=threshold_optimal)\n",
    "\n",
    "    print(f'The column indices of the selected features for fold {fold} are: \\n {selected_indices}')\n",
    "\n",
    "    # Determine names of features that were selected\n",
    "    features_selected = list()\n",
    "    for index in selected_indices:\n",
    "        features_selected.append(DATA_FEAT_SEL.columns[index])\n",
    "    FEATURES_SELECTED_TOTAL.append(features_selected)\n",
    "\n",
    "    # -------------- INNER CROSSVALIDATION FOR DETERMINATION OF HYPERPARAMETERS OF CLASSIFIERS ---------------------------------\n",
    "    svm_classifier = svm_randomized_search(X_train_sel, y_train)   \n",
    "\n",
    "    rf_classifier = rf_randomized_search(X_train_sel, y_train)\n",
    "\n",
    "    knn_classifier = knn_randomized_search(X_train_sel, y_train)\n",
    "\n",
    "    clfs = [svm_classifier, rf_classifier, knn_classifier]\n",
    "    clfs_names = ['SVM', 'RF', 'KNN']\n",
    "\n",
    "    # ---------------- TRAIN AND TEST CLASSIFIERS (OUTER CROSSVALIDATION) ------------------------------------------------\n",
    "    # Initialisation\n",
    "    acc_dict = dict()\n",
    "    accuracy_total = list()\n",
    "\n",
    "    bal_acc_dict = dict()\n",
    "    bal_acc_total = list()\n",
    "\n",
    "    roc_auc_score_dict = dict()\n",
    "    roc_auc_score_total = list()\n",
    "\n",
    "    # Determine performance\n",
    "    for clf, name in zip(clfs, clfs_names):\n",
    "        acc_dict[name], bal_acc_dict[name], roc_auc_score_dict[name] = get_performance_classifier(clf, X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "    accuracy_total.append(acc_dict)\n",
    "    bal_acc_total.append(bal_acc_dict)\n",
    "    roc_auc_score_total.append(roc_auc_score_dict)\n",
    "    \n",
    "    ACCURACY = ACCURACY.append(acc_dict, ignore_index=True)\n",
    "    ACCURACY_BALANCED = ACCURACY_BALANCED.append(bal_acc_dict, ignore_index=True)\n",
    "    AUC = AUC.append(roc_auc_score_dict, ignore_index=True)\n",
    "\n",
    "    print(f'Regular accuracy: \\n {acc_dict}')\n",
    "    print(f'Balanced accuracy: \\n {bal_acc_dict}')\n",
    "    print(f'Roc auc score: \\n {roc_auc_score_dict}')\n",
    "\n",
    "    #-------------------- LEARNING CURVES --------------------------------------------------\n",
    "    # Initialisation\n",
    "    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=None)\n",
    "    num = 0\n",
    "    fig = plt.figure(figsize=(24,8*len(clfs)))\n",
    "\n",
    "    # Merge LGG and GBM patients\n",
    "    X_total = np.concatenate((X_train_sel, X_test_sel), axis=0)\n",
    "    y_total = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    # Compute learning curves\n",
    "    for clf in clfs:\n",
    "        title = str(type(clf))\n",
    "        ax = fig.add_subplot(7, 3, num+1)\n",
    "        plot_learning_curve(clf, title, X_total, y_total, ax, ylim=(0.3, 1.01), cv=cv, train_sizes=np.linspace(0.2,                             1,5))\n",
    "        num += 1\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The mean accuracy per classifier is: \nSVM    0.818717\nRF     0.873440\nKNN    0.885383\ndtype: float64\nThe mean balanced accuracy per classifier is: \nSVM    0.808645\nRF     0.846026\nKNN    0.862930\ndtype: float64\nThe mean AUC per classifier is: \nSVM    0.850635\nRF     0.930733\nKNN    0.892821\ndtype: float64\n"
    }
   ],
   "source": [
    "# Determine mean accuracy, balanced accuracy and AUC score per classifier\n",
    "MEAN_ACC = ACCURACY.mean()\n",
    "print(f'The mean accuracy per classifier is: \\n{MEAN_ACC}')\n",
    "MEAN_ACC_BAL = ACCURACY_BALANCED.mean()\n",
    "print(f'The mean balanced accuracy per classifier is: \\n{MEAN_ACC_BAL}')\n",
    "MEAN_AUC = AUC.mean()\n",
    "print(f'The mean AUC per classifier is: \\n{MEAN_AUC}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}