{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "outputs": [],
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from brats.load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The number of samples: 167\nThe number of columns: 725\n"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The feature TGM_Cog_X_6 does not have any values in the GBM dataset\nThe feature TGM_Cog_Y_6 does not have any values in the GBM dataset\nThe feature TGM_Cog_Z_6 does not have any values in the GBM dataset\nThe feature TGM_T_6 does not have any values in the GBM dataset\n"
    }
   ],
   "source": [
    "# Splitting the data into GBM and LGG\n",
    "\n",
    "GBM = data.loc[data['label'] == 'GBM']\n",
    "LGG = data.loc[data['label'] == 'LGG']\n",
    "\n",
    "# check if a feature has at least one value per label\n",
    "\n",
    "empty_feature = []\n",
    "\n",
    "for labels, content in GBM.iteritems():\n",
    "    if content.count() == 0:\n",
    "        empty_feature.append(labels)\n",
    "        print(f'The feature {labels} does not have any values in the GBM dataset')\n",
    "\n",
    "for labels, content in LGG.iteritems():\n",
    "    if content.count() == 0:\n",
    "        empty_feature.append(labels)\n",
    "        print(f'The feature {labels} does not have any values in the LGG dataset')\n",
    "\n",
    "# remove the features that have no data for at least one label\n",
    "\n",
    "data_empty_removed = data.drop(columns=[feature for feature in set(empty_feature)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of NaN row-wise and columnwise\n",
    "# Column counting is done on the full data, and for GBM and LGG seperately\n",
    "# It can be usefull to see from which the class the NaN's come from\n",
    "# This should/might be taken into account when selecting the features\n",
    "\n",
    "import pandas as pd\n",
    "no_nan_row = data.isnull().sum(axis=1)\n",
    "no_nan_col = data.isnull().sum(axis=0)\n",
    "GBM_no_nan_col = GBM.isnull().sum(axis=0)\n",
    "LGG_no_nan_col = LGG.isnull().sum(axis=0)\n",
    "\n",
    "frame = { 'Total (n=167)': no_nan_col, 'GBM (n=102)': GBM_no_nan_col, 'LGG (n=65)': LGG_no_nan_col } \n",
    "overview_nan = pd.DataFrame(frame)\n",
    "\n",
    "# The variables (series) below 'bins' the NaN's:\n",
    "# - the index column is the amount of NaN's in the dataset \n",
    "# - the second column is the amount of features that have this amount of NaN's\n",
    "\n",
    "aantal_NAN_GBM = GBM_no_nan_col.value_counts()\n",
    "aantal_NAN_LGG = LGG_no_nan_col.value_counts()\n",
    "aantal_NAN_total = no_nan_col.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of the GBM dataset\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "GBM_imputed = data_empty_removed.loc[data_empty_removed['label'] == 'GBM'].drop(columns=['label'])\n",
    "array_imp_GBM = imputer.fit_transform(GBM_imputed)\n",
    "GBM_imputed[:] = array_imp_GBM\n",
    "GBM_imputed['label'] = 'GBM'\n",
    "\n",
    "\n",
    "# Imputation of the LGG dataset\n",
    "\n",
    "LGG_imputed = data_empty_removed.loc[data_empty_removed['label'] == 'LGG'].drop(columns=['label'])\n",
    "array_imp_LGG = imputer.fit_transform(LGG_imputed)\n",
    "LGG_imputed[:] = array_imp_LGG\n",
    "LGG_imputed['label'] = 'LGG'\n",
    "\n",
    "# Combining the datasets again\n",
    "\n",
    "data_imputed = GBM_imputed\n",
    "data_imputed = data_imputed.append(LGG_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}