{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "outputs": [],
   "source": [
    "# TM10007 Assignment: Prediction of Tumor Grade in Brain Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 4: Kiefer Comassi (4402359), Myrthe van Haaften (4547470), Frédérique Koopman (4470885), Stephanie Stoutjesdijk (4557808)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing functions and packages"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git\n",
    "#!pip install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from missingpy import KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import svm\n",
    "\n",
    "from brats.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting data "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_data()\n",
    "\n",
    "# Splitting feature values and patient labels\n",
    "FEATURES = data.drop(columns=['label'])\n",
    "LABELS = data['label']\n",
    "\n",
    "GBM = FEATURES.loc[LABELS=='GBM']\n",
    "LGG = FEATURES.loc[LABELS=='LGG']\n",
    "\n",
    "# Splitting into train and test set --> dit moet weg\n",
    "#X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(FEATURES, LABELS, test_size=0.2, random_state=42)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing before crossvalidation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the NaN's in the dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of NaN's\n",
    "NO_NAN_ROW_TOTAL = FEATURES.isnull().sum(axis=1)             # Number of NaN's per patient for GBM and LGG patients\n",
    "NO_NAN_COL_TOTAL = FEATURES.isnull().sum(axis=0)             # Number of NaN's per feature for GBM and LGG patients\n",
    "\n",
    "GBM_NO_NAN_COL = GBM.isnull().sum(axis=0)                    # Number of NaN's per feature for GBM patients\n",
    "LGG_NO_NAN_COL = LGG.isnull().sum(axis=0)                    # Number of NaN's per feature for LGG patients\n",
    "OVERVIEW_NAN = { 'Total': NO_NAN_COL_TOTAL, 'GBM': GBM_NO_NAN_COL, 'LGG': LGG_NO_NAN_COL } \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection based on the number of NaN's. Threshold = the maximum number of NaN's in a column"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage of patients with no data for a certain feature, above which the feature is discarded\n",
    "PERC_MISSING_GBM = 30\n",
    "PERC_MISSING_LGG = 30\n",
    "\n",
    "# Determining threshold for discarding feature based on above percentage\n",
    "THRESHOLD_GBM = floor((PERC_MISSING_GBM/100) * len(GBM.index))\n",
    "THRESHOLD_LGG = floor((PERC_MISSING_LGG/100) * len(LGG.index))\n",
    "\n",
    "# Initialisation\n",
    "FEATURES_REMOVED = []\n",
    "\n",
    "# Append names of features that should be discarded to list\n",
    "\n",
    "for feature in GBM_NO_NAN_COL[GBM_NO_NAN_COL > THRESHOLD_GBM].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "for feature in LGG_NO_NAN_COL[LGG_NO_NAN_COL > THRESHOLD_LGG].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "# Remove features from dataset\n",
    "DATA_FEAT_SEL = FEATURES.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "\n",
    "# The variables (series) below 'bins' the NaN's:\n",
    "# - the index column is the amount of NaN's in the dataset \n",
    "# - the second column is the amount of features that have this amount of NaN's\n",
    "\n",
    "#aantal_NAN_GBM = GBM_no_nan_col.value_counts()\n",
    "#aantal_NAN_LGG = LGG_no_nan_col.value_counts()\n",
    "#aantal_NAN_total = no_nan_col.value_counts()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patient selection based on the number of NaN's. Threshold = the maximum number of NaN's in a row."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient selection \n",
    "\n",
    "# Percentage/number of features that a patient is allowed to miss. When above this amount, this patient is removed from the trainingset, because it is missing too many features. \n",
    "PERC_MISSING_SAMPLE = 30\n",
    "\n",
    "# Make the threshold\n",
    "THRESHOLD_SAMPLE = floor((PERC_MISSING_SAMPLE/100) * len(DATA_FEAT_SEL.columns))\n",
    "\n",
    "# Number of NaN's per patient AFTER removing some features\n",
    "NO_NAN_ROW_TRAIN = DATA_FEAT_SEL.isnull().sum(axis=1)      \n",
    "\n",
    "# Make an empty list, which will be filled with samples that are above the threshold\n",
    "SAMPLES_REMOVED = [] \n",
    "LABELS_SEL = LABELS\n",
    "\n",
    "# Looping over the trainingset to determine which patients are above the threshold, and remove them directly.\n",
    "#print('These samples are removed from dataset:')\n",
    "#for sample in NO_NAN_ROW_TRAIN[NO_NAN_ROW_TRAIN > THRESHOLD_SAMPLE].index[:]:\n",
    "#    if sample:\n",
    "#        print(sample)\n",
    "#        SAMPLES_REMOVED.append(sample)      # This should be removed in combination with the comment before\n",
    "#        DATA_FEAT_SEL = DATA_FEAT_SEL.drop(index=sample)\n",
    "#        LABELS_SEL = LABELS_SEL.drop(index=sample)\n",
    "\n",
    "#print(f'{len(DATA_FEAT_SEL)}/{len(FEATURES)} samples are left in dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of data distribution and outliers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data distribution: \n",
    "IMPUTER_TOTAL = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "DATA_TOTAL_IMPUTED = IMPUTER_TOTAL.fit_transform(DATA_FEAT_SEL)\n",
    "\n",
    "NO_NORM_DISTR = 0\n",
    "FEAT_NORM = list()\n",
    "for index, feature in enumerate(DATA_TOTAL_IMPUTED.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value>0.05:\n",
    "        NO_NORM_DISTR += 1 \n",
    "        FEAT_NORM.append(index)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate per patient each feature\n",
    "NO_OUTLIERS = 0\n",
    "\n",
    "for patient in DATA_TOTAL_IMPUTED:\n",
    "    feat_out = 0\n",
    "    for index, feature in enumerate(patient):\n",
    "        q25, q75 = np.percentile(DATA_TOTAL_IMPUTED[:,index], 25), np.percentile(DATA_TOTAL_IMPUTED[:,index], 75)\n",
    "        iqr = q75 - q25\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        outlier = ((feature>upper) | (feature<lower))\n",
    "        feat_out += outlier\n",
    "    if feat_out > 70:\n",
    "        NO_OUTLIERS += 1\n",
    "\n",
    "# Calculate mean and standard deviation per feature based on training data\n",
    "#MEAN = DATA_TOTAL_IMPUTED.mean(axis=0)\n",
    "#STD = DATA_TOTAL_IMPUTED.std(axis=0)\n",
    "\n",
    "# Evaluate per patient each feature\n",
    "#NO_OUTLIERS = 0\n",
    "#for patient in DATA_TOTAL_IMPUTED:\n",
    "#    feat_out = 0\n",
    "#    for feature, mu, std in zip(patient, MEAN, STD):\n",
    "#        outlier = ((feature>mu+3*std) | (feature<mu-3*std))\n",
    "#        feat_out += outlier\n",
    "#    if feat_out > 30:\n",
    "#        no_outliers += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer crossvalidation loop\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "OPTIMAL_NUMBER_FEATURES = list()\n",
    "\n",
    "for train_index, test_index in skf.split(DATA_FEAT_SEL, LABELS_SEL):\n",
    "    X_TRAIN, X_TEST = np.array(DATA_FEAT_SEL)[train_index], np.array(DATA_FEAT_SEL)[test_index]\n",
    "    Y_TRAIN, Y_TEST = np.array(LABELS_SEL)[train_index], np.array(LABELS_SEL)[test_index]\n",
    "\n",
    "    GBM_TRAIN = X_TRAIN[Y_TRAIN=='GBM']\n",
    "    LGG_TRAIN = X_TRAIN[Y_TRAIN=='LGG']\n",
    "\n",
    "    # ---------------------------------- IMPUTATION -----------------------------------\n",
    "    # Definition of imputers\n",
    "    IMPUTER_GBM = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    IMPUTER_LGG = KNNImputer(n_neighbors=5, weights=\"uniform\")        \n",
    "    IMPUTER_TEST = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "    # Impute GBM train set\n",
    "    X_TRAIN_FEAT_SEL_GBM = X_TRAIN[Y_TRAIN=='GBM']        \n",
    "    X_TRAIN_IMP_GBM = IMPUTER_GBM.fit_transform(X_TRAIN_FEAT_SEL_GBM)    \n",
    "\n",
    "    # Impute LGG train set\n",
    "    X_TRAIN_FEAT_SEL_LGG = X_TRAIN[Y_TRAIN=='LGG']        \n",
    "    X_TRAIN_IMP_LGG = IMPUTER_LGG.fit_transform(X_TRAIN_FEAT_SEL_LGG)    \n",
    "\n",
    "    # Merge imputed GBM and LGG arrays\n",
    "    X_TRAIN_IMP_TOT = np.concatenate((X_TRAIN_IMP_GBM, X_TRAIN_IMP_LGG))\n",
    "                                        \n",
    "    #LGG_IMPUTED[:] = ARRAY_IMP_LGG                                                      # Overwrite original values with imputed values in dataframe\n",
    "    #LGG_IMPUTED['label'] = 'LGG'                                                        # Add column containing label\n",
    "\n",
    "    # Impute test set \n",
    "    IMPUTER_TEST.fit(X_TRAIN_IMP_TOT)\n",
    "    X_TEST_IMP = IMPUTER_TEST.transform(X_TEST)\n",
    " \n",
    "    # ------------------------------------ SCALING ----------------------------------------\n",
    "    SCALER = RobustScaler()\n",
    "    SCALER.fit(X_TRAIN_IMP_TOT)\n",
    "    X_TRAIN_SCAL = SCALER.transform(X_TRAIN_IMP_TOT)\n",
    "    X_TEST_SCAL = SCALER.transform(X_TEST_IMP)\n",
    "\n",
    "\n",
    "    # ------------------------------------ FEATURE SELECTION -------------------------------\n",
    "    svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "    # classifications\n",
    "    rfecv = RFECV(\n",
    "        estimator=svc, step=1, \n",
    "        cv=StratifiedKFold(4),\n",
    "        scoring='roc_auc')\n",
    "    rfecv.fit(X_TRAIN_SCAL, Y_TRAIN)                               \n",
    "\n",
    "    OPTIMAL_NUMBER_FEATURES.append(rfecv.n_features_)\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "\n",
    "    X_TRAIN_FINAL = X_TRAIN_SCAL[:,rfecv.support_]\n",
    "    X_TEST_FINAL = X_TEST_SCAL[:,rfecv.support_]\n",
    "    #print(rfecv.ranking_)\n",
    "#np.absolute(rfecv.estimator_.coef_)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #KNN Classification\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',metric_params=None, n_jobs=1)\n",
    "    knn.fit(X_TRAIN_SCAL, Y_TRAIN)\n",
    "    #predict the response for test dataset\n",
    "    y_pred = knn.predict(X_TEST_SCAL)\n",
    "    from sklearn import metrics \n",
    "    print(\"Accuracy:\",metrics.accuracy_score(Y_TEST, y_pred))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Random Forest Classification\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf=RandomForestClassifier(n_estimators=128)\n",
    "    clf.fit(X_TRAIN_SCAL, Y_TRAIN)\n",
    "    y_pred=clf.predict(X_TEST_SCAL)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(Y_TEST, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning SVM\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid =  {'kernel': ['linear', 'rbf', 'poly'],\n",
    "                'C': [0.01, 0.1, 1, 10, 100],\n",
    "                'gamma': [1, 0.1, 0.01, 0.001, 'auto', 'scale'],\n",
    "                'degree': [1, 2, 3, 4, 5],\n",
    "                'coef0': [0.01, 0.5, 1, 5, 10, 20]} \n",
    "\n",
    "GRID = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
    "GRID.fit(X_TRAIN_FINAL, Y_TRAIN)\n",
    "\n",
    "SVM_CLASSIFIER = GRID.best_estimator_\n",
    "print(SVM_CLASSIFIER)\n",
    "RESULTS = pd.DataFrame(GRID.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}