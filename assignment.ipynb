{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "outputs": [],
   "source": [
    "# TM10007 Assignment: Prediction of Tumor Grade in Brain Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 4: Kiefer Comassi (4402359), Myrthe van Haaften (4547470), Frédérique Koopman (4470885), Stephanie Stoutjesdijk (4557808)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing functions and packages"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git\n",
    "#!pip install missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from missingpy import KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import svm\n",
    "\n",
    "from brats.load_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting data "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_data()\n",
    "\n",
    "# Splitting feature values and patient labels\n",
    "FEATURES = data.drop(columns=['label'])\n",
    "LABELS = data['label']\n",
    "\n",
    "# Splitting into train and test set\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(FEATURES, LABELS, test_size=0.2, random_state=42)\n",
    "\n",
    "GBM_TRAIN = X_TRAIN.loc[Y_TRAIN=='GBM']\n",
    "LGG_TRAIN = X_TRAIN.loc[Y_TRAIN=='LGG']\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer crossvalidation loop\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "for train_index, test_index in skf.split(FEATURES, LABELS):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = np.array(FEATURES)[train_index], np.array(FEATURES)[test_index]\n",
    "        y_train, y_test = np.array(LABELS)[train_index], np.array(LABELS)[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the NaN's in the dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the number of NaN's\n",
    "NO_NAN_ROW_TOTAL = X_TRAIN.isnull().sum(axis=1)             # Number of NaN's per patient for GBM and LGG patients\n",
    "NO_NAN_COL_TOTAL = X_TRAIN.isnull().sum(axis=0)             # Number of NaN's per feature for GBM and LGG patients\n",
    "\n",
    "GBM_NO_NAN_COL = GBM_TRAIN.isnull().sum(axis=0)             # Number of NaN's per feature for GBM patients\n",
    "LGG_NO_NAN_COL = LGG_TRAIN.isnull().sum(axis=0)             # Number of NaN's per feature for LGG patients\n",
    "OVERVIEW_NAN = { 'Total': NO_NAN_COL_TOTAL, 'GBM': GBM_NO_NAN_COL, 'LGG': LGG_NO_NAN_COL } \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection based on the number of NaN's. Threshold = the maximum number of NaN's in a column."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage of patients with no data for a certain feature, above which the feature is discarded\n",
    "PERC_MISSING_GBM = 30\n",
    "PERC_MISSING_LGG = 30\n",
    "\n",
    "# Determining threshold for discarding feature based on above percentage\n",
    "THRESHOLD_GBM = floor((PERC_MISSING_GBM/100) * len(GBM_TRAIN.index))\n",
    "THRESHOLD_LGG = floor((PERC_MISSING_LGG/100) * len(LGG_TRAIN.index))\n",
    "\n",
    "# Initialisation\n",
    "FEATURES_REMOVED = []\n",
    "\n",
    "# Append names of features that should be discarded to list\n",
    "\n",
    "for feature in GBM_NO_NAN_COL[GBM_NO_NAN_COL > THRESHOLD_GBM].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "for feature in LGG_NO_NAN_COL[LGG_NO_NAN_COL > THRESHOLD_LGG].index[:]:\n",
    "    FEATURES_REMOVED.append(feature)\n",
    "\n",
    "# Remove features from dataset\n",
    "X_TRAIN_FEAT_SEL = X_TRAIN.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "X_TEST_FEAT_SEL = X_TEST.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "\n",
    "# The variables (series) below 'bins' the NaN's:\n",
    "# - the index column is the amount of NaN's in the dataset \n",
    "# - the second column is the amount of features that have this amount of NaN's\n",
    "\n",
    "#aantal_NAN_GBM = GBM_no_nan_col.value_counts()\n",
    "#aantal_NAN_LGG = LGG_no_nan_col.value_counts()\n",
    "#aantal_NAN_total = no_nan_col.value_counts()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "These samples are removed from trainingset:\nTCGA-DU-A5TU\nTCGA-DU-8162\nTCGA-CS-6669\nTCGA-HT-7602\nTCGA-DU-5874\nTCGA-DU-8166\nTCGA-DU-7019\nTCGA-DU-7299\nTCGA-CS-6666\nTCGA-FG-6691\nTCGA-FG-7634\nTCGA-HT-7884\nTCGA-HT-7686\nTCGA-CS-4944\nTCGA-HT-A61A\nTCGA-CS-5397\n117/133 samples are left in trainingset\n117\n"
    }
   ],
   "source": [
    "# Patient selection in the trainingset\n",
    "\n",
    "# Percentage/number of features that a patient is allowed to miss. When above this amount, this patient is removed from the trainingset, because it is missing too many features. \n",
    "PERC_MISSING_SAMPLE = 30\n",
    "\n",
    "# Make the threshold\n",
    "THRESHOLD_SAMPLE = floor((PERC_MISSING_SAMPLE/100) * len(X_TRAIN_FEAT_SEL.columns))\n",
    "\n",
    "# Number of NaN's per patient AFTER removing some features\n",
    "NO_NAN_ROW_TRAIN = X_TRAIN_FEAT_SEL.isnull().sum(axis=1)      \n",
    "\n",
    "# Make an empty list, which will be filled with samples that are above the threshold\n",
    "SAMPLES_REMOVED_TRAIN = []\n",
    "\n",
    "Y_TRAIN_SAMP_REM = Y_TRAIN \n",
    "\n",
    "# Looping over the trainingset to determine which patients are above the threshold, and remove them directly.\n",
    "print('These samples are removed from trainingset:')\n",
    "for sample in NO_NAN_ROW_TRAIN[NO_NAN_ROW_TRAIN > THRESHOLD_SAMPLE].index[:]:\n",
    "    print(sample)\n",
    "    SAMPLES_REMOVED_TRAIN.append(sample)      # This should be removed in combination with the comment before\n",
    "    X_TRAIN_FEAT_SEL = X_TRAIN_FEAT_SEL.drop(index=sample)\n",
    "    Y_TRAIN_SAMP_REM = Y_TRAIN_SAMP_REM.drop(index=sample)\n",
    "\n",
    "print(f'{len(X_TRAIN_FEAT_SEL)}/{len(X_TRAIN)} samples are left in trainingset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nThese samples are removed from testset:\nTCGA-CS-6665\nTCGA-HT-7879\nTCGA-DU-8164\nTCGA-DU-A5TS\nTCGA-DU-7008\n29/34 samples are left in testset\n"
    }
   ],
   "source": [
    "# Patient selection in the testset (Threshold and percentage of missing values used from patient selection trainingset)\n",
    "\n",
    "# Number of NaN's per patient AFTER removing some features\n",
    "NO_NAN_ROW_TEST = X_TEST_FEAT_SEL.isnull().sum(axis=1)   \n",
    "\n",
    "# Make an empty list, which will be filled with samples that are above the threshold \n",
    "SAMPLES_REMOVED_TEST = []\n",
    "\n",
    "Y_TEST_SAMP_REM = Y_TEST\n",
    "\n",
    "# Looping over the trainingset to determine which patients are above the threshold, and remove them directly.\n",
    "print('\\nThese samples are removed from testset:')\n",
    "for sample in NO_NAN_ROW_TEST[NO_NAN_ROW_TEST > THRESHOLD_SAMPLE].index[:]:\n",
    "    print(sample)\n",
    "    SAMPLES_REMOVED_TEST.append(sample)      # This should be removed in combination with the comment before\n",
    "    X_TEST_FEAT_SEL = X_TEST_FEAT_SEL.drop(index=sample)\n",
    "    Y_TEST_SAMP_REM = Y_TEST_SAMP_REM.drop(index=sample)\n",
    "\n",
    "print(f'{len(X_TEST_FEAT_SEL)}/{len(X_TEST)} samples are left in testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of imputers\n",
    "IMPUTER_GBM = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "IMPUTER_LGG = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "IMPUTER_TEST = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "# Impute GBM train set\n",
    "X_TRAIN_FEAT_SEL_GBM = X_TRAIN_FEAT_SEL.loc[Y_TRAIN_SAMP_REM=='GBM']        #    <=======\n",
    "X_TRAIN_IMP_GBM = IMPUTER_GBM.fit_transform(X_TRAIN_FEAT_SEL_GBM)    \n",
    "\n",
    "# Impute LGG train set\n",
    "X_TRAIN_FEAT_SEL_LGG = X_TRAIN_FEAT_SEL.loc[Y_TRAIN_SAMP_REM=='LGG']        #    <=======\n",
    "X_TRAIN_IMP_LGG = IMPUTER_LGG.fit_transform(X_TRAIN_FEAT_SEL_LGG)    \n",
    "\n",
    "# Merge imputed GBM and LGG arrays\n",
    "X_TRAIN_IMP_TOT = np.concatenate((X_TRAIN_IMP_GBM, X_TRAIN_IMP_LGG))\n",
    "                                \n",
    "#LGG_IMPUTED[:] = ARRAY_IMP_LGG                                                      # Overwrite original values with imputed values in dataframe\n",
    "#LGG_IMPUTED['label'] = 'LGG'                                                        # Add column containing label\n",
    "\n",
    "# Impute test set \n",
    "IMPUTER_TEST.fit(X_TRAIN_IMP_TOT)\n",
    "X_TEST_IMP = IMPUTER_TEST.transform(X_TEST_FEAT_SEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of data distribution and outliers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data distribution: \n",
    "IMPUTER_TOTAL = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "\n",
    "FEATURES_SEL = FEATURES.drop(columns=[features for features in set(FEATURES_REMOVED)])\n",
    "FEATURES_SEL = FEATURES_SEL.drop(index=[sample for sample in set(SAMPLES_REMOVED_TEST)])   #    <=======\n",
    "FEATURES_SEL = FEATURES_SEL.drop(index=[sample for sample in set(SAMPLES_REMOVED_TRAIN)])  #    <=======\n",
    "DATA_TOTAL_IMPUTED = IMPUTER_TOTAL.fit_transform(FEATURES_SEL)\n",
    "\n",
    "NO_NORM_DISTR = 0\n",
    "FEAT_NORM = list()\n",
    "for index, feature in enumerate(DATA_TOTAL_IMPUTED.T):\n",
    "    t_stat, p_value = shapiro(feature)\n",
    "    if p_value>0.05:\n",
    "        NO_NORM_DISTR += 1 \n",
    "        FEAT_NORM.append(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate per patient each feature\n",
    "NO_OUTLIERS = 0\n",
    "\n",
    "for patient in DATA_TOTAL_IMPUTED:\n",
    "    feat_out = 0\n",
    "    for index, feature in enumerate(patient):\n",
    "        if index in FEAT_NORM:\n",
    "            mu = DATA_TOTAL_IMPUTED[:,index].mean()\n",
    "            std = DATA_TOTAL_IMPUTED[:,index].std()\n",
    "            outlier = ((feature>mu+3*std) | (feature<mu-3*std))\n",
    "            feat_out += outlier\n",
    "        else:\n",
    "           q25, q75 = np.percentile(DATA_TOTAL_IMPUTED[:,index], 25), np.percentile(DATA_TOTAL_IMPUTED[:,index], 75)\n",
    "           iqr = q75 - q25\n",
    "           cut_off = iqr * 1.5\n",
    "           lower, upper = q25 - cut_off, q75 + cut_off\n",
    "           outlier = ((feature>upper) | (feature<lower))\n",
    "           feat_out += outlier\n",
    "    if feat_out > 70:\n",
    "        NO_OUTLIERS += 1\n",
    "\n",
    "# Calculate mean and standard deviation per feature based on training data\n",
    "#MEAN = DATA_TOTAL_IMPUTED.mean(axis=0)\n",
    "#STD = DATA_TOTAL_IMPUTED.std(axis=0)\n",
    "\n",
    "# Evaluate per patient each feature\n",
    "#NO_OUTLIERS = 0\n",
    "#for patient in DATA_TOTAL_IMPUTED:\n",
    "#    feat_out = 0\n",
    "#    for feature, mu, std in zip(patient, MEAN, STD):\n",
    "#        outlier = ((feature>mu+3*std) | (feature<mu-3*std))\n",
    "#        feat_out += outlier\n",
    "#    if feat_out > 30:\n",
    "#        no_outliers += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling of data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = RobustScaler()\n",
    "SCALER.fit(X_TRAIN_IMP_TOT)\n",
    "X_TRAIN_SCAL = SCALER.transform(X_TRAIN_IMP_TOT)\n",
    "X_TEST_SCAL = SCALER.transform(X_TEST_IMP)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "# classifications\n",
    "rfecv = RFECV(\n",
    "    estimator=svc, step=1, \n",
    "    cv=StratifiedKFold(4),\n",
    "    scoring='roc_auc')\n",
    "rfecv.fit(X_TRAIN_SCAL, Y_TRAIN_SAMP_REM)                               #    <=======\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "X_TRAIN_FINAL = X_TRAIN_SCAL[:,rfecv.support_]\n",
    "X_TEST_FINAL = X_TEST_SCAL[:,rfecv.support_]\n",
    "#print(rfecv.ranking_)\n",
    "#np.absolute(rfecv.estimator_.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}